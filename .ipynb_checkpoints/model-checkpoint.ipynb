{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://smerity.com/articles/2016/google_nmt_arch.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_cell: <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x123bdfa20>\n",
      "decoder_initial_state(AttentionWrapperState, cell_state, c or h)\n",
      "decoder_initial_state length(layer_num):  8\n",
      "decoder_initial_state[0][0][0].get_shape():  (12, 128)\n",
      "decoder_initial_state[0][0][1].get_shape():  (12, 128)\n"
     ]
    }
   ],
   "source": [
    "TOKEN_PAD=\"<p>\"\n",
    "TOKEN_START=\"<s>\"\n",
    "TOKEN_END=\"</s>\"\n",
    "\n",
    "extra_tokens = [TOKEN_PAD, TOKEN_START, TOKEN_END]\n",
    "pad_token = extra_tokens.index(TOKEN_PAD)\n",
    "start_token = extra_tokens.index(TOKEN_START)\n",
    "end_token = extra_tokens.index(TOKEN_END)\n",
    "\n",
    "src_vocab_size = 200\n",
    "tgt_vocab_size = 200\n",
    "\n",
    "embedding_size = 128\n",
    "hidden_units = 128\n",
    "layer_num = 8\n",
    "\n",
    "beam_width = 3\n",
    "batch_size = 12\n",
    "\n",
    "encoder_residual_start_idx = 3\n",
    "decoder_residual_start_idx = 2\n",
    "\n",
    "is_train_mode = True\n",
    "use_beamsearch_decode = False\n",
    "\n",
    "# Placeholder\n",
    "'''\n",
    "inputs: [batch_size, max_time_steps]\n",
    "inputs_length: [batch_size]\n",
    "'''\n",
    "encoder_inputs = tf.placeholder(dtype=tf.int32, shape=(batch_size, None), name='encoder_inputs')\n",
    "encoder_inputs_length = tf.placeholder(dtype=tf.int32, shape=(batch_size,), name='encoder_inputs_length')\n",
    "\n",
    "decoder_inputs = tf.placeholder(dtype=tf.int32, shape=(batch_size, None), name='decoder_inputs')\n",
    "decoder_inputs_length = tf.placeholder(dtype=tf.int32, shape=(batch_size,), name='decoder_inputs_length')\n",
    "decoder_inputs_length_train = decoder_inputs_length + 1\n",
    "\n",
    "decoder_start_token = tf.ones(shape=[batch_size, 1], dtype=tf.int32) * start_token\n",
    "decoder_end_token = tf.ones(shape=[batch_size, 1], dtype=tf.int32) * end_token  \n",
    "decoder_targets = tf.concat([decoder_inputs, decoder_end_token], axis=1)\n",
    "\n",
    "keep_probability = tf.placeholder(dtype=tf.float32, shape=[], name='keep_probability')\n",
    "\n",
    "# Embedding\n",
    "with tf.variable_scope('embedding_layer'):\n",
    "    \n",
    "    sqrt3 = math.sqrt(3)\n",
    "    initializer = tf.random_uniform_initializer(-sqrt3, sqrt3, dtype=tf.float32)\n",
    "    \n",
    "    embedding_encoder = tf.get_variable(name=\"embedding_encoder\",\n",
    "                                        shape=[src_vocab_size, embedding_size], \n",
    "                                        dtype=tf.float32,\n",
    "                                        initializer=initializer,\n",
    "                                        trainable=True)\n",
    "    encoder_embeddding_inputs = tf.nn.embedding_lookup(params=embedding_encoder,\n",
    "                                                       ids=encoder_inputs)\n",
    "    \n",
    "    embedding_decoder = tf.get_variable(name=\"embedding_decoder\",\n",
    "                                        shape=[tgt_vocab_size, embedding_size], \n",
    "                                        dtype=tf.float32,\n",
    "                                        initializer=initializer,\n",
    "                                        trainable=True)\n",
    "    decoder_embeddding_inputs = tf.nn.embedding_lookup(params=embedding_decoder,\n",
    "                                                       ids=decoder_inputs)\n",
    "\n",
    "\n",
    "\n",
    "def print_tuple_state(tuple_state):\n",
    "    print('len(layer_num): ', len(tuple_state))\n",
    "    for state in tuple_state:\n",
    "        if len(state) > 1:\n",
    "            print('c:', state[0].get_shape())\n",
    "            print('h:', state[1].get_shape())  \n",
    "        else:\n",
    "            print(state.get_shape())\n",
    "    print('\\n')\n",
    "    return tuple_state\n",
    "    \n",
    "def build_single_cell(hidden_units, keep_probability, use_residual=True):\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(hidden_units)\n",
    "    cell = tf.contrib.rnn.DropoutWrapper(cell, \n",
    "                                         dtype=tf.float32,\n",
    "                                         output_keep_prob=keep_probability)\n",
    "    if use_residual:\n",
    "        cell = tf.contrib.rnn.ResidualWrapper(cell)\n",
    "    return cell\n",
    "\n",
    "def attn_decoder_input_fn(inputs, attention):\n",
    "    # Essential when use_residual=True\n",
    "    #print('attn_decoder_input_fn - inputs:', inputs)\n",
    "    #print('attn_decoder_input_fn - attention:', attention)\n",
    "\n",
    "    attn_decoder_input = tf.concat([inputs, attention], -1)\n",
    "    attn_decoder_input = tf.layers.dense(attn_decoder_input, hidden_units, name='attn_decoder_input')\n",
    "    return attn_decoder_input\n",
    "    \n",
    "# Projection Layer\n",
    "with tf.variable_scope('projection_layer'):\n",
    "    input_layer = tf.layers.Dense(hidden_units, dtype=tf.float32, name='input_projection')\n",
    "    output_layer = tf.layers.Dense(tgt_vocab_size, dtype=tf.float32, name='output_projection')\n",
    "\n",
    "# Encoder\n",
    "with tf.variable_scope('encoder'):\n",
    "    \n",
    "    encoder_embeddding_inputs = input_layer(encoder_embeddding_inputs)\n",
    "        \n",
    "    forward_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_units)\n",
    "    backward_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_units)\n",
    "    \n",
    "    bi_outputs, bi_state = tf.nn.bidirectional_dynamic_rnn(forward_cell, \n",
    "                                                            backward_cell, \n",
    "                                                            encoder_embeddding_inputs,\n",
    "                                                            sequence_length=encoder_inputs_length,\n",
    "                                                            dtype=tf.float32,\n",
    "                                                            time_major=False)\n",
    "    bi_encoder_outputs = tf.concat(bi_outputs, -1)\n",
    "    \n",
    "    encoder_cell_list = [build_single_cell(hidden_units, keep_probability, use_residual=False)]\n",
    "    encoder_cell_list.extend([build_single_cell(hidden_units, keep_probability) for i in range(layer_num - encoder_residual_start_idx)])\n",
    "    \n",
    "    encoder_cell = tf.contrib.rnn.MultiRNNCell(encoder_cell_list)\n",
    "    \n",
    "    encoder_outputs, encoder_final_state = tf.nn.dynamic_rnn(cell=encoder_cell,\n",
    "                                                       inputs=bi_encoder_outputs,\n",
    "                                                       sequence_length=encoder_inputs_length,\n",
    "                                                       dtype=tf.float32,\n",
    "                                                       time_major=False)\n",
    "\n",
    "\n",
    "# Decoder\n",
    "with tf.variable_scope('decoder'):\n",
    "    \n",
    "    # train\n",
    "    if is_train_mode:\n",
    "        decoder_cell_list = [build_single_cell(hidden_units, keep_probability, use_residual=True) for i in range(decoder_residual_start_idx)]\n",
    "        decoder_cell_list.extend([build_single_cell(hidden_units, keep_probability) for i in range(layer_num - decoder_residual_start_idx)])\n",
    "        attention_mechanism = tf.contrib.seq2seq.LuongAttention(num_units=hidden_units,\n",
    "                                                            memory=encoder_outputs, \n",
    "                                                            memory_sequence_length=encoder_inputs_length)\n",
    "\n",
    "        \n",
    "        for i in range(len(decoder_cell_list)):\n",
    "            decoder_cell_list[i] = tf.contrib.seq2seq.AttentionWrapper(\n",
    "                cell=decoder_cell_list[i],\n",
    "                attention_mechanism=attention_mechanism,\n",
    "                attention_layer_size=hidden_units,\n",
    "                cell_input_fn=attn_decoder_input_fn,\n",
    "                alignment_history=False,\n",
    "                name='Attention_Wrapper')\n",
    "        \n",
    "        decoder_cell = tf.contrib.rnn.MultiRNNCell(decoder_cell_list)\n",
    "        \n",
    "        decoder_initial_state = decoder_cell.zero_state(batch_size=batch_size, dtype=tf.float32)\n",
    "        decoder_initial_state = tuple(decoder_initial_state) \n",
    "\n",
    "        print('decoder_cell:', decoder_cell)\n",
    "        print('decoder_initial_state(AttentionWrapperState, cell_state, c or h)')\n",
    "        print('decoder_initial_state length(layer_num): ', len(decoder_initial_state))\n",
    "        print('decoder_initial_state[0][0][0].get_shape(): ', decoder_initial_state[0][0][0].get_shape())\n",
    "        print('decoder_initial_state[0][0][1].get_shape(): ', decoder_initial_state[0][0][1].get_shape())\n",
    "\n",
    "        decoder_embeddding_inputs = input_layer(decoder_embeddding_inputs)\n",
    "        training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=decoder_embeddding_inputs,\n",
    "                                                            sequence_length=decoder_inputs_length,\n",
    "                                                            time_major=False,\n",
    "                                                            name='training_helper')\n",
    "        \n",
    "        training_decoder = tf.contrib.seq2seq.BasicDecoder(cell=decoder_cell,\n",
    "                                                           helper=training_helper,\n",
    "                                                           initial_state=decoder_initial_state,\n",
    "                                                           output_layer=output_layer)\n",
    "        \n",
    "        max_decoder_length = tf.reduce_max(decoder_inputs_length_train)\n",
    "        decoder_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder=training_decoder,\n",
    "                                                              output_time_major=False,\n",
    "                                                              impute_finished=True,\n",
    "                                                              maximum_iterations=max_decoder_length)\n",
    "\n",
    "\n",
    "        crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=decoder_targets, \n",
    "                                                                  logits=decoder_outputs.rnn_output)\n",
    "        masks = tf.sequence_mask(lengths=decoder_inputs_length_train, \n",
    "                                 maxlen=max_decoder_length, \n",
    "                                 dtype=tf.float32, \n",
    "                                 name='masks')\n",
    "        loss = (tf.reduce_sum(crossent * masks) / batch_size)\n",
    "        tf.summary.scalar('loss', loss)\n",
    "        \n",
    "        \n",
    "    # inference\n",
    "    else:\n",
    "        \n",
    "        if use_beamsearch_decode:\n",
    "            encoder_outputs = tf.contrib.seq2seq.tile_batch(encoder_outputs, multiplier=beam_width) \n",
    "            encoder_final_state = tf.contrib.framework.nest.map_structure(\n",
    "                lambda state: tf.contrib.seq2seq.tile_batch(state, beam_width), \n",
    "                encoder_final_state)\n",
    "            encoder_inputs_length = tf.contrib.seq2seq.tile_batch(encoder_inputs_length, multiplier=beam_width)\n",
    "            #print_tuple_state(encoder_final_state)\n",
    "\n",
    "            \n",
    "        decoder_cell_list = [build_single_cell(hidden_units, keep_probability, use_residual=True) for i in range(decoder_residual_start_idx)]\n",
    "        decoder_cell_list.extend([build_single_cell(hidden_units, keep_probability) for i in range(layer_num - decoder_residual_start_idx)])\n",
    "        \n",
    "        attention_mechanism = tf.contrib.seq2seq.LuongAttention(num_units=hidden_units,\n",
    "                                                                memory=encoder_outputs, \n",
    "                                                                memory_sequence_length=encoder_inputs_length)\n",
    "            \n",
    "        decoder_initial_state = []\n",
    "        for i in range(len(decoder_cell_list)):\n",
    "            decoder_cell_list[i] = tf.contrib.seq2seq.AttentionWrapper(\n",
    "                cell=decoder_cell_list[i],\n",
    "                attention_mechanism=attention_mechanism,\n",
    "                attention_layer_size=hidden_units,\n",
    "                cell_input_fn=attn_decoder_input_fn,\n",
    "                alignment_history=False,\n",
    "                name='Attention_Wrapper')\n",
    "                \n",
    "                \n",
    "        decoder_cell = tf.contrib.rnn.MultiRNNCell(decoder_cell_list)\n",
    "        \n",
    "        if not use_beamsearch_decode:\n",
    "            beam_batch_size = batch_size\n",
    "        else:\n",
    "            beam_batch_size = batch_size * beam_width\n",
    "        \n",
    "        \n",
    "        decoder_initial_state = decoder_cell.zero_state(batch_size=beam_batch_size, dtype=tf.float32)\n",
    "        decoder_initial_state = tuple(decoder_initial_state) \n",
    "\n",
    "        print('decoder_cell:', decoder_cell)\n",
    "        print('decoder_initial_state(AttentionWrapperState, cell_state, c or h)')\n",
    "        print('decoder_initial_state length(layer_num): ', len(decoder_initial_state))\n",
    "        print('decoder_initial_state[0][0][0].get_shape(): ', decoder_initial_state[0][0][0].get_shape())\n",
    "        print('decoder_initial_state[0][0][1].get_shape(): ', decoder_initial_state[0][0][1].get_shape())\n",
    "        \n",
    "        \n",
    "        start_tokens = tf.fill([batch_size], start_token)\n",
    "        if not use_beamsearch_decode:\n",
    "            decoding_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(start_tokens=start_tokens,\n",
    "                                                                       end_token=end_token,\n",
    "                                                                       embedding=embedding_decoder)\n",
    "            \n",
    "            inference_decoder = tf.contrib.seq2seq.BasicDecoder(cell=decoder_cell,\n",
    "                                                               helper=decoding_helper,\n",
    "                                                               initial_state=decoder_initial_state,\n",
    "                                                               output_layer=output_layer)\n",
    "        else:\n",
    "            inference_decoder = tf.contrib.seq2seq.BeamSearchDecoder(cell=decoder_cell,\n",
    "                                                                     embedding=embedding_decoder,\n",
    "                                                                     start_tokens=start_tokens,\n",
    "                                                                     end_token=end_token,\n",
    "                                                                     initial_state=decoder_initial_state,\n",
    "                                                                     beam_width=beam_width,\n",
    "                                                                     output_layer=output_layer,)\n",
    "\n",
    "        max_decoder_length = tf.reduce_max(decoder_inputs_length_train)\n",
    "        decoder_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder=inference_decoder,\n",
    "                                                                  output_time_major=False,\n",
    "                                                                  maximum_iterations=max_decoder_length)\n",
    "\n",
    "\n",
    "\n",
    "        if not use_beamsearch_decode:\n",
    "            decoder_pred_decode = tf.expand_dims(decoder_outputs.sample_id, -1)\n",
    "        else:\n",
    "            decoder_pred_decode = decoder_outputs.predicted_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "GNMT Attention Cell로는 Bhadanau style을 사용함\n",
    "attention_layer_size = None => attention layer 사용안함, Bhadanau style\n",
    "output_attention = False => output으로 attention이 아닌 cell output을 사용함, Bhadanau style\n",
    "'''\n",
    "attention_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "        attention_cell,\n",
    "        attention_mechanism,\n",
    "        attention_layer_size=None,  # don't use attention layer.\n",
    "        output_attention=False,\n",
    "        alignment_history=alignment_history,\n",
    "        name=\"attention\")\n",
    "\n",
    "    if attention_architecture == \"gnmt\":\n",
    "      cell = GNMTAttentionMultiCell(\n",
    "          attention_cell, cell_list)\n",
    "    elif attention_architecture == \"gnmt_v2\":\n",
    "      cell = GNMTAttentionMultiCell(\n",
    "          attention_cell, cell_list, use_new_attention=True)\n",
    "\n",
    "    \n",
    "    \n",
    "# https://github.com/tensorflow/nmt/blob/master/nmt/gnmt_model.py\n",
    "# 가져다가 쓰면 Decoder쪽에  GNMT Attention MultiCell로 사용 \n",
    "# GNMT에서는 맨 아래의 셀만 AttentionWrapper로 감싸주고 나머지는 Cell은 state만\n",
    "# 복사해서 input과 concat\n",
    "class GNMTAttentionMultiCell(tf.nn.rnn_cell.MultiRNNCell):\n",
    "  \"\"\"A MultiCell with GNMT attention style.\"\"\"\n",
    "\n",
    "  def __init__(self, attention_cell, cells, use_new_attention=False):\n",
    "    \"\"\"Creates a GNMTAttentionMultiCell.\n",
    "    Args:\n",
    "      attention_cell: An instance of AttentionWrapper.\n",
    "      cells: A list of RNNCell wrapped with AttentionInputWrapper.\n",
    "      use_new_attention: Whether to use the attention generated from current\n",
    "        step bottom layer's output. Default is False.\n",
    "    \"\"\"\n",
    "    cells = [attention_cell] + cells\n",
    "    self.use_new_attention = use_new_attention\n",
    "    super(GNMTAttentionMultiCell, self).__init__(cells, state_is_tuple=True)\n",
    "\n",
    "  def __call__(self, inputs, state, scope=None):\n",
    "    \"\"\"Run the cell with bottom layer's attention copied to all upper layers.\"\"\"\n",
    "    if not nest.is_sequence(state):\n",
    "      raise ValueError(\n",
    "          \"Expected state to be a tuple of length %d, but received: %s\"\n",
    "          % (len(self.state_size), state))\n",
    "\n",
    "    with tf.variable_scope(scope or \"multi_rnn_cell\"):\n",
    "      new_states = []\n",
    "\n",
    "      with tf.variable_scope(\"cell_0_attention\"):\n",
    "        attention_cell = self._cells[0] #AttentionWrapper Cell\n",
    "        attention_state = state[0] # AttentionWrapper Cell의 이전(t-1) attention_state\n",
    "        \n",
    "        #현재(t) Input과 이전(t-1) attention_state 를 넣으면 \n",
    "        #cur_inp, new_attention_state 가 나오는데 \n",
    "        #여기서 cur_inp은 cell아웃풋이면서 attention 값이고, \n",
    "        #마찬가지로 new_attention_state안에는 next_cell_state(Cell state)과 attention이 둘 다 있음\n",
    "        cur_inp, new_attention_state = attention_cell(inputs, attention_state) \n",
    "        new_states.append(new_attention_state)\n",
    "\n",
    "    # AttentionWrapper Cell위에 쌓여있는 cell들에게는\n",
    "    # input값으로 cur_input과 첫번째레이어(AttentionWrapper)의 attenion값이 같이 들어감\n",
    "      for i in range(1, len(self._cells)):\n",
    "        with tf.variable_scope(\"cell_%d\" % i):\n",
    "\n",
    "          cell = self._cells[i]\n",
    "          cur_state = state[i]\n",
    "            \n",
    "        \n",
    "          if self.use_new_attention:\n",
    "            cur_inp = tf.concat([cur_inp, new_attention_state.attention], -1)\n",
    "          else:\n",
    "            cur_inp = tf.concat([cur_inp, attention_state.attention], -1)\n",
    "\n",
    "          cur_inp, new_state = cell(cur_inp, cur_state)\n",
    "          new_states.append(new_state)\n",
    "\n",
    "    return cur_inp, tuple(new_states)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
