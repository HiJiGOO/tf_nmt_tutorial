{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3, Minibatch Loss= 3.480366, Training Accuracy= 0.00000\n",
      "Step 300, Minibatch Loss= 2.834722, Training Accuracy= 0.20000\n",
      "Step 600, Minibatch Loss= 2.340374, Training Accuracy= 0.20000\n",
      "Step 900, Minibatch Loss= 2.201402, Training Accuracy= 0.20000\n",
      "Step 1200, Minibatch Loss= 2.180447, Training Accuracy= 0.20000\n",
      "Step 1500, Minibatch Loss= 2.172209, Training Accuracy= 0.20000\n",
      "Step 1800, Minibatch Loss= 2.161288, Training Accuracy= 0.20000\n",
      "Step 2100, Minibatch Loss= 2.139388, Training Accuracy= 0.20000\n",
      "Step 2400, Minibatch Loss= 2.134336, Training Accuracy= 0.20000\n",
      "Step 2700, Minibatch Loss= 2.132357, Training Accuracy= 0.20000\n",
      "Step 3000, Minibatch Loss= 2.131060, Training Accuracy= 0.20000\n",
      "Step 3300, Minibatch Loss= 2.130099, Training Accuracy= 0.26667\n",
      "Step 3600, Minibatch Loss= 2.129343, Training Accuracy= 0.26667\n",
      "Step 3900, Minibatch Loss= 2.128729, Training Accuracy= 0.26667\n",
      "Step 4200, Minibatch Loss= 2.128216, Training Accuracy= 0.26667\n",
      "Step 4500, Minibatch Loss= 2.127779, Training Accuracy= 0.26667\n",
      "Step 4800, Minibatch Loss= 2.127402, Training Accuracy= 0.26667\n",
      "Step 5100, Minibatch Loss= 2.127074, Training Accuracy= 0.26667\n",
      "Step 5400, Minibatch Loss= 2.126784, Training Accuracy= 0.26667\n",
      "Step 5700, Minibatch Loss= 2.126527, Training Accuracy= 0.26667\n",
      "Step 6000, Minibatch Loss= 2.126297, Training Accuracy= 0.26667\n",
      "Step 6300, Minibatch Loss= 2.126090, Training Accuracy= 0.26667\n",
      "Step 6600, Minibatch Loss= 2.125903, Training Accuracy= 0.26667\n",
      "Step 6900, Minibatch Loss= 2.125733, Training Accuracy= 0.26667\n",
      "Step 7200, Minibatch Loss= 2.125578, Training Accuracy= 0.26667\n",
      "Step 7500, Minibatch Loss= 2.125436, Training Accuracy= 0.26667\n",
      "Step 7800, Minibatch Loss= 2.125306, Training Accuracy= 0.26667\n",
      "Step 8100, Minibatch Loss= 2.125186, Training Accuracy= 0.26667\n",
      "Step 8400, Minibatch Loss= 2.125076, Training Accuracy= 0.26667\n",
      "Step 8700, Minibatch Loss= 2.124973, Training Accuracy= 0.26667\n",
      "Step 9000, Minibatch Loss= 2.124878, Training Accuracy= 0.26667\n",
      "Step 9300, Minibatch Loss= 2.124790, Training Accuracy= 0.26667\n",
      "Step 9600, Minibatch Loss= 2.124708, Training Accuracy= 0.26667\n",
      "Step 9900, Minibatch Loss= 2.124631, Training Accuracy= 0.26667\n",
      "Step 10200, Minibatch Loss= 2.124561, Training Accuracy= 0.26667\n",
      "Step 10500, Minibatch Loss= 2.124494, Training Accuracy= 0.26667\n",
      "Step 10800, Minibatch Loss= 2.124432, Training Accuracy= 0.26667\n",
      "Step 11100, Minibatch Loss= 2.124374, Training Accuracy= 0.26667\n",
      "Step 11400, Minibatch Loss= 2.124319, Training Accuracy= 0.26667\n",
      "Step 11700, Minibatch Loss= 2.124268, Training Accuracy= 0.26667\n",
      "Step 12000, Minibatch Loss= 2.124220, Training Accuracy= 0.26667\n",
      "Step 12300, Minibatch Loss= 2.124174, Training Accuracy= 0.26667\n",
      "Step 12600, Minibatch Loss= 2.124131, Training Accuracy= 0.26667\n",
      "Step 12900, Minibatch Loss= 2.124091, Training Accuracy= 0.26667\n",
      "Step 13200, Minibatch Loss= 2.124053, Training Accuracy= 0.26667\n",
      "Step 13500, Minibatch Loss= 2.124016, Training Accuracy= 0.26667\n",
      "Step 13800, Minibatch Loss= 2.123981, Training Accuracy= 0.26667\n",
      "Step 14100, Minibatch Loss= 2.123948, Training Accuracy= 0.26667\n",
      "Step 14400, Minibatch Loss= 2.123917, Training Accuracy= 0.26667\n",
      "Step 14700, Minibatch Loss= 2.123887, Training Accuracy= 0.26667\n",
      "Step 15000, Minibatch Loss= 2.123858, Training Accuracy= 0.26667\n",
      "Step 15300, Minibatch Loss= 2.123831, Training Accuracy= 0.26667\n",
      "Step 15600, Minibatch Loss= 2.123805, Training Accuracy= 0.26667\n",
      "Step 15900, Minibatch Loss= 2.123780, Training Accuracy= 0.26667\n",
      "Step 16200, Minibatch Loss= 2.123756, Training Accuracy= 0.26667\n",
      "Step 16500, Minibatch Loss= 2.123733, Training Accuracy= 0.26667\n",
      "Step 16800, Minibatch Loss= 2.123711, Training Accuracy= 0.26667\n",
      "Step 17100, Minibatch Loss= 2.123690, Training Accuracy= 0.26667\n",
      "Step 17400, Minibatch Loss= 2.123670, Training Accuracy= 0.26667\n",
      "Step 17700, Minibatch Loss= 2.123650, Training Accuracy= 0.26667\n",
      "Step 18000, Minibatch Loss= 2.123631, Training Accuracy= 0.26667\n",
      "Step 18300, Minibatch Loss= 2.123614, Training Accuracy= 0.26667\n",
      "Step 18600, Minibatch Loss= 2.123596, Training Accuracy= 0.26667\n",
      "Step 18900, Minibatch Loss= 2.123580, Training Accuracy= 0.20000\n",
      "Step 19200, Minibatch Loss= 2.123564, Training Accuracy= 0.20000\n",
      "Step 19500, Minibatch Loss= 2.123549, Training Accuracy= 0.20000\n",
      "Step 19800, Minibatch Loss= 2.123534, Training Accuracy= 0.20000\n",
      "Step 20100, Minibatch Loss= 2.123520, Training Accuracy= 0.20000\n",
      "Step 20400, Minibatch Loss= 2.123506, Training Accuracy= 0.20000\n",
      "Step 20700, Minibatch Loss= 2.123492, Training Accuracy= 0.20000\n",
      "Step 21000, Minibatch Loss= 2.123479, Training Accuracy= 0.20000\n",
      "Step 21300, Minibatch Loss= 2.123466, Training Accuracy= 0.20000\n",
      "Step 21600, Minibatch Loss= 2.123447, Training Accuracy= 0.20000\n",
      "Step 21900, Minibatch Loss= 2.123432, Training Accuracy= 0.20000\n",
      "Step 22200, Minibatch Loss= 2.123418, Training Accuracy= 0.20000\n",
      "Step 22500, Minibatch Loss= 2.123406, Training Accuracy= 0.20000\n",
      "Step 22800, Minibatch Loss= 2.123394, Training Accuracy= 0.20000\n",
      "Step 23100, Minibatch Loss= 2.123384, Training Accuracy= 0.20000\n",
      "Step 23400, Minibatch Loss= 2.123373, Training Accuracy= 0.20000\n",
      "Step 23700, Minibatch Loss= 2.123363, Training Accuracy= 0.20000\n",
      "Step 24000, Minibatch Loss= 2.123354, Training Accuracy= 0.20000\n",
      "Step 24300, Minibatch Loss= 2.123345, Training Accuracy= 0.20000\n",
      "Step 24600, Minibatch Loss= 2.123336, Training Accuracy= 0.20000\n",
      "Step 24900, Minibatch Loss= 2.123328, Training Accuracy= 0.20000\n",
      "Step 25200, Minibatch Loss= 2.123320, Training Accuracy= 0.20000\n",
      "Step 25500, Minibatch Loss= 2.123312, Training Accuracy= 0.20000\n",
      "Step 25800, Minibatch Loss= 2.123304, Training Accuracy= 0.20000\n",
      "Step 26100, Minibatch Loss= 2.123297, Training Accuracy= 0.20000\n",
      "Step 26400, Minibatch Loss= 2.123290, Training Accuracy= 0.20000\n",
      "Step 26700, Minibatch Loss= 2.123283, Training Accuracy= 0.20000\n",
      "Step 27000, Minibatch Loss= 2.123276, Training Accuracy= 0.20000\n",
      "Step 27300, Minibatch Loss= 2.123270, Training Accuracy= 0.20000\n",
      "Step 27600, Minibatch Loss= 2.123264, Training Accuracy= 0.20000\n",
      "Step 27900, Minibatch Loss= 2.123257, Training Accuracy= 0.20000\n",
      "Step 28200, Minibatch Loss= 2.123252, Training Accuracy= 0.20000\n",
      "Step 28500, Minibatch Loss= 2.123246, Training Accuracy= 0.20000\n",
      "Step 28800, Minibatch Loss= 2.123240, Training Accuracy= 0.20000\n",
      "Step 29100, Minibatch Loss= 2.123235, Training Accuracy= 0.20000\n",
      "Step 29400, Minibatch Loss= 2.123230, Training Accuracy= 0.20000\n",
      "Step 29700, Minibatch Loss= 2.123225, Training Accuracy= 0.20000\n",
      "Step 30000, Minibatch Loss= 2.123220, Training Accuracy= 0.20000\n",
      "Step 30300, Minibatch Loss= 2.123215, Training Accuracy= 0.20000\n",
      "Step 30600, Minibatch Loss= 2.123211, Training Accuracy= 0.20000\n",
      "Step 30900, Minibatch Loss= 2.123206, Training Accuracy= 0.20000\n",
      "Step 31200, Minibatch Loss= 2.123201, Training Accuracy= 0.20000\n",
      "Step 31500, Minibatch Loss= 2.123197, Training Accuracy= 0.20000\n",
      "Step 31800, Minibatch Loss= 2.123192, Training Accuracy= 0.20000\n",
      "Step 32100, Minibatch Loss= 2.123187, Training Accuracy= 0.20000\n",
      "Step 32400, Minibatch Loss= 2.123179, Training Accuracy= 0.20000\n",
      "Step 32700, Minibatch Loss= 2.123172, Training Accuracy= 0.20000\n",
      "Step 33000, Minibatch Loss= 2.123167, Training Accuracy= 0.20000\n",
      "Step 33300, Minibatch Loss= 2.123163, Training Accuracy= 0.20000\n",
      "Step 33600, Minibatch Loss= 2.123159, Training Accuracy= 0.20000\n",
      "Step 33900, Minibatch Loss= 2.123156, Training Accuracy= 0.20000\n",
      "Step 34200, Minibatch Loss= 2.123152, Training Accuracy= 0.20000\n",
      "Step 34500, Minibatch Loss= 2.123149, Training Accuracy= 0.20000\n",
      "Step 34800, Minibatch Loss= 2.123146, Training Accuracy= 0.20000\n",
      "Step 35100, Minibatch Loss= 2.123143, Training Accuracy= 0.20000\n",
      "Step 35400, Minibatch Loss= 2.123140, Training Accuracy= 0.20000\n",
      "Step 35700, Minibatch Loss= 2.123137, Training Accuracy= 0.20000\n",
      "Step 36000, Minibatch Loss= 2.123135, Training Accuracy= 0.20000\n",
      "Step 36300, Minibatch Loss= 2.123132, Training Accuracy= 0.20000\n",
      "Step 36600, Minibatch Loss= 2.123130, Training Accuracy= 0.20000\n",
      "Step 36900, Minibatch Loss= 2.123128, Training Accuracy= 0.20000\n",
      "Step 37200, Minibatch Loss= 2.123126, Training Accuracy= 0.20000\n",
      "Step 37500, Minibatch Loss= 2.123123, Training Accuracy= 0.20000\n",
      "Step 37800, Minibatch Loss= 2.123122, Training Accuracy= 0.20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 38100, Minibatch Loss= 2.123119, Training Accuracy= 0.20000\n",
      "Step 38400, Minibatch Loss= 2.123117, Training Accuracy= 0.20000\n",
      "Step 38700, Minibatch Loss= 2.123115, Training Accuracy= 0.20000\n",
      "Step 39000, Minibatch Loss= 2.123113, Training Accuracy= 0.20000\n",
      "Step 39300, Minibatch Loss= 2.123111, Training Accuracy= 0.20000\n",
      "Step 39600, Minibatch Loss= 2.123109, Training Accuracy= 0.20000\n",
      "Step 39900, Minibatch Loss= 2.123107, Training Accuracy= 0.20000\n",
      "Step 40200, Minibatch Loss= 2.123105, Training Accuracy= 0.20000\n",
      "Step 40500, Minibatch Loss= 2.123103, Training Accuracy= 0.20000\n",
      "Step 40800, Minibatch Loss= 2.123101, Training Accuracy= 0.20000\n",
      "Step 41100, Minibatch Loss= 2.123100, Training Accuracy= 0.20000\n",
      "Step 41400, Minibatch Loss= 2.123098, Training Accuracy= 0.20000\n",
      "Step 41700, Minibatch Loss= 2.123097, Training Accuracy= 0.20000\n",
      "Step 42000, Minibatch Loss= 2.123095, Training Accuracy= 0.20000\n",
      "Step 42300, Minibatch Loss= 2.123093, Training Accuracy= 0.20000\n",
      "Step 42600, Minibatch Loss= 2.123092, Training Accuracy= 0.20000\n",
      "Step 42900, Minibatch Loss= 2.123090, Training Accuracy= 0.20000\n",
      "Step 43200, Minibatch Loss= 2.123089, Training Accuracy= 0.20000\n",
      "Step 43500, Minibatch Loss= 2.123087, Training Accuracy= 0.20000\n",
      "Step 43800, Minibatch Loss= 2.123086, Training Accuracy= 0.20000\n",
      "Step 44100, Minibatch Loss= 2.123085, Training Accuracy= 0.20000\n",
      "Step 44400, Minibatch Loss= 2.123084, Training Accuracy= 0.20000\n",
      "Step 44700, Minibatch Loss= 2.123082, Training Accuracy= 0.20000\n",
      "Step 45000, Minibatch Loss= 2.123081, Training Accuracy= 0.20000\n",
      "Step 45300, Minibatch Loss= 2.123080, Training Accuracy= 0.20000\n",
      "Step 45600, Minibatch Loss= 2.123079, Training Accuracy= 0.20000\n",
      "Step 45900, Minibatch Loss= 2.123078, Training Accuracy= 0.20000\n",
      "Step 46200, Minibatch Loss= 2.123076, Training Accuracy= 0.20000\n",
      "Step 46500, Minibatch Loss= 2.123075, Training Accuracy= 0.20000\n",
      "Step 46800, Minibatch Loss= 2.123074, Training Accuracy= 0.20000\n",
      "Step 47100, Minibatch Loss= 2.123073, Training Accuracy= 0.20000\n",
      "Step 47400, Minibatch Loss= 2.123072, Training Accuracy= 0.20000\n",
      "Step 47700, Minibatch Loss= 2.123071, Training Accuracy= 0.20000\n",
      "Step 48000, Minibatch Loss= 2.123070, Training Accuracy= 0.20000\n",
      "Step 48300, Minibatch Loss= 2.123069, Training Accuracy= 0.20000\n",
      "Step 48600, Minibatch Loss= 2.123068, Training Accuracy= 0.20000\n",
      "Step 48900, Minibatch Loss= 2.123067, Training Accuracy= 0.20000\n",
      "Step 49200, Minibatch Loss= 2.123066, Training Accuracy= 0.20000\n",
      "Step 49500, Minibatch Loss= 2.123065, Training Accuracy= 0.20000\n",
      "Step 49800, Minibatch Loss= 2.123065, Training Accuracy= 0.20000\n",
      "Step 50100, Minibatch Loss= 2.123064, Training Accuracy= 0.20000\n",
      "Step 50400, Minibatch Loss= 2.123063, Training Accuracy= 0.20000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-227bcef1f83a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    131\u001b[0m         sess.run(optimizer, feed_dict={source_inputs: batch_source_input, \n\u001b[1;32m    132\u001b[0m                                        \u001b[0mtarget_inputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_target_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m                                        target_outputs: batch_target_output})\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdisplay_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "TOKEN_PAD=\"<p>\"\n",
    "TOKEN_START=\"<s>\"\n",
    "TOKEN_END=\"</s>\"\n",
    "\n",
    "source_sentences = [\"취미가 뭐예요\", \"만나서 반가워요\", \"내일 만나서 놀아요\"]\n",
    "target_sentences = [\"what is your hobby\", \"nice to meet you\", \"meet tomorrow\"]\n",
    "\n",
    "source_vocab = [TOKEN_PAD, TOKEN_START, TOKEN_END, \"취미가\", \"뭐예요\", \"만나서\", \"반가워요\", \"내일\", \"놀아요\"]\n",
    "target_vocab = [TOKEN_PAD, TOKEN_START, TOKEN_END, \"what\", \"is\", \"your\", \"hobby\", \"nice\", \"to\", \"meet\", \"you\", \"tomorrow\"]\n",
    "\n",
    "source_input_idx = [\n",
    "    [3,  4,  0,  0,  0], \n",
    "    [5,  6,  0,  0,  0],  \n",
    "    [7,  5,  8,  0,  0]]\n",
    "target_input_idx = [\n",
    "    [1,  3,  4,  5,  6], \n",
    "    [1,  7,  8,  9, 10], \n",
    "    [1,  9, 11,  0,  0]]\n",
    "target_output_idx = [\n",
    "    [3,  4,  5,  6,  2], \n",
    "    [7,  8,  9, 10,  2], \n",
    "    [9, 11,  2,  0,  0]]\n",
    "    \n",
    "source_vocab_size = len(source_vocab)\n",
    "target_vocab_size = len(target_vocab)\n",
    "\n",
    "embedding_size = 12\n",
    "num_units = 12\n",
    "num_layer = 1\n",
    "\n",
    "batch_size = 3\n",
    "learning_rate = 0.0001\n",
    "\n",
    "training_steps = 40000\n",
    "display_step = 200\n",
    "\n",
    "max_sentence_length = 5\n",
    "                   \n",
    "source_inputs = tf.placeholder(dtype=tf.int64, shape=(batch_size, max_sentence_length), name='source_inputs')\n",
    "target_inputs = tf.placeholder(dtype=tf.int64, shape=(batch_size, max_sentence_length), name='target_inputs')\n",
    "target_outputs = tf.placeholder(dtype=tf.int64, shape=(batch_size, max_sentence_length), name='target_outputs')\n",
    "\n",
    "def build_single_cell(num_units):\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(num_units)\n",
    "    return cell\n",
    "                   \n",
    "with tf.variable_scope('encoder'):\n",
    "    \n",
    "    initializer = tf.contrib.layers.xavier_initializer()\n",
    "    embedding_encoder = tf.get_variable(name=\"embedding_encoder\",\n",
    "                                        shape=[source_vocab_size, embedding_size], \n",
    "                                        dtype=tf.float32,\n",
    "                                        initializer=initializer,\n",
    "                                        trainable=True)\n",
    "    \n",
    "    encoder_embeddding_inputs = tf.nn.embedding_lookup(params=embedding_encoder,\n",
    "                                                       ids=source_inputs)\n",
    "\n",
    "    encoder_cell_list = [build_single_cell(num_units) for i in range(num_layer)]\n",
    "    encoder_multi_cell = tf.contrib.rnn.MultiRNNCell(encoder_cell_list)\n",
    "    \n",
    "    encoder_outputs, encoder_final_state = tf.nn.dynamic_rnn(cell=encoder_multi_cell,\n",
    "                                                             inputs=encoder_embeddding_inputs,\n",
    "                                                             dtype=tf.float32)\n",
    "    \n",
    "with tf.variable_scope('decoder'):\n",
    "    \n",
    "    initializer = tf.contrib.layers.xavier_initializer()\n",
    "    embedding_decoder = tf.get_variable(name=\"embedding_decoder\",\n",
    "                                        shape=[target_vocab_size, embedding_size], \n",
    "                                        dtype=tf.float32,\n",
    "                                        initializer=initializer,\n",
    "                                        trainable=True)\n",
    "    \n",
    "    decoder_embeddding_inputs = tf.nn.embedding_lookup(params=embedding_decoder,\n",
    "                                                       ids=target_inputs)\n",
    "    \n",
    "    decoder_cell_list = [build_single_cell(num_units) for i in range(num_layer)]\n",
    "        \n",
    "    # Attention mechanism #\n",
    "    attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(num_units=num_units, \n",
    "                                                              memory=encoder_outputs) \n",
    "    ######################\n",
    "    \n",
    "    # Attention mechanism #\n",
    "    decoder_cell_list[-1] = tf.contrib.seq2seq.AttentionWrapper(\n",
    "            cell=decoder_cell_list[-1],\n",
    "            attention_mechanism=attention_mechanism,\n",
    "            initial_cell_state=encoder_final_state[-1],\n",
    "            name='Attention_Wrapper')\n",
    "    ######################\n",
    "    \n",
    "    # Attention mechanism #\n",
    "    decoder_initial_state = [decoder_cell_list[-1].zero_state(batch_size=batch_size, dtype=tf.float32)]\n",
    "    decoder_initial_state[0].clone(cell_state=encoder_final_state[-1])\n",
    "    decoder_initial_state = tuple(decoder_initial_state)\n",
    "    ######################\n",
    "    \n",
    "#     decoder_initial_state = encoder_final_state\n",
    "    \n",
    "    decoder_multi_cell = tf.contrib.rnn.MultiRNNCell(decoder_cell_list)\n",
    "    decoder_outputs, decoder_final_state = tf.nn.dynamic_rnn(cell=decoder_multi_cell,\n",
    "                                                             inputs=decoder_embeddding_inputs,\n",
    "                                                             initial_state=decoder_initial_state,\n",
    "                                                             dtype=tf.float32)\n",
    "\n",
    "    decoder_predict = tf.argmax(decoder_outputs, 2)\n",
    "    \n",
    "with tf.variable_scope(\"optimizer\"):\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=decoder_outputs,\n",
    "                                                            labels=target_outputs)\n",
    "    cost = tf.reduce_mean(cross_entropy)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    optimizer = optimizer.minimize(cost)\n",
    "    \n",
    "    correct_pred = tf.equal(tf.argmax(decoder_outputs, 2), target_outputs)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    \n",
    "    \n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(1, training_steps + 1):\n",
    "        batch_source_input = source_input_idx\n",
    "        batch_target_input = target_input_idx\n",
    "        batch_target_output = target_output_idx\n",
    "        \n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(optimizer, feed_dict={source_inputs: batch_source_input, \n",
    "                                       target_inputs: batch_target_input,\n",
    "                                       target_outputs: batch_target_output})\n",
    "        \n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch accuracy & loss\n",
    "            outputs, acc, loss = sess.run([decoder_predict, accuracy, cost], feed_dict={source_inputs: batch_source_input, \n",
    "                                                              target_inputs: batch_target_input,\n",
    "                                                              target_outputs: batch_target_output})\n",
    "            \n",
    "            print(\"Step \" + str(step * batch_size) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc))\n",
    "            \n",
    "        if acc >= 1:\n",
    "            for sentence in outputs:\n",
    "                sentence = [target_vocab[word_idx] for word_idx in sentence]\n",
    "                print(\"           -> \", sentence)\n",
    "                \n",
    "            break;\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={source_inputs: source_input_idx, \n",
    "                                                             target_inputs: target_input_idx,\n",
    "                                                             target_outputs: target_output_idx}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
