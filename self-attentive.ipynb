{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tflearn/data_utils.py:201: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "INFO:tensorflow:Restoring parameters from ./save/self-attentive/self-attentive.ckpt\n",
      "[[1.09844325e-14 1.10143497e-14 1.10173338e-14 5.45164872e-14\n",
      "  1.11669512e-14 1.10015002e-14 5.08892516e-14 9.88555610e-01\n",
      "  1.31500704e-08 1.32851841e-14 2.57025965e-14 1.09848527e-14\n",
      "  1.10125862e-14 1.09863612e-14 1.09848527e-14 1.09843910e-14\n",
      "  1.09843910e-14 1.09843910e-14 1.09843910e-14 1.09843910e-14\n",
      "  1.09843910e-14 1.09844325e-14 2.98499018e-14 2.98558243e-14\n",
      "  1.14444476e-02 1.09843910e-14 1.09843910e-14 1.09844325e-14\n",
      "  1.09843910e-14 1.09843910e-14]\n",
      " [3.37292958e-14 3.37292958e-14 3.37294212e-14 3.37293568e-14\n",
      "  3.37292958e-14 3.37307730e-14 3.37297430e-14 3.37294855e-14\n",
      "  3.37608325e-14 3.37399752e-14 3.37434514e-14 3.12903661e-11\n",
      "  2.83569612e-10 9.78894949e-01 2.11050063e-02 1.30270577e-11\n",
      "  3.37425468e-14 3.38109667e-14 3.38879925e-14 3.37332193e-14\n",
      "  3.37316099e-14 3.37293568e-14 3.37292958e-14 3.37292958e-14\n",
      "  3.37292958e-14 3.37292958e-14 3.37859555e-14 3.37857590e-14\n",
      "  3.37294212e-14 3.37293568e-14]\n",
      " [4.80306772e-15 4.80304908e-15 4.80304908e-15 4.80310456e-15\n",
      "  4.80534031e-15 4.80328752e-15 4.84075687e-15 1.35003482e-14\n",
      "  1.31734841e-14 1.79126114e-02 9.82087314e-01 5.71997960e-09\n",
      "  3.66344596e-14 4.86474823e-15 4.80457035e-15 4.80849423e-15\n",
      "  4.80413074e-15 4.80308593e-15 4.80367250e-15 4.80315962e-15\n",
      "  4.80306772e-15 4.80334258e-15 4.80330573e-15 4.80337900e-15\n",
      "  4.81491855e-15 4.80389230e-15 4.80341585e-15 4.84822304e-15\n",
      "  4.80963180e-15 4.80449708e-15]\n",
      " [4.10415500e-15 4.10415500e-15 4.10423335e-15 4.10446798e-15\n",
      "  4.10449932e-15 4.10454633e-15 4.10611207e-15 4.10454633e-15\n",
      "  4.10442097e-15 4.10608073e-15 4.10484364e-15 4.10420201e-15\n",
      "  4.10432695e-15 4.10418634e-15 4.10415500e-15 4.10415500e-15\n",
      "  4.10415500e-15 4.10415500e-15 4.10415500e-15 4.10415500e-15\n",
      "  4.10415500e-15 4.10415500e-15 4.10415500e-15 1.11554832e-14\n",
      "  1.11563557e-14 1.19177096e-01 8.80822957e-01 4.93037700e-09\n",
      "  4.10418634e-15 4.10417067e-15]\n",
      " [4.31307483e-15 4.31312395e-15 4.31309134e-15 4.35837584e-15\n",
      "  4.31480277e-15 4.31519791e-15 5.06901362e-15 4.31320654e-15\n",
      "  4.31312395e-15 4.32427726e-15 4.31307483e-15 4.31307483e-15\n",
      "  4.31381513e-15 4.31307483e-15 4.31307483e-15 4.31307483e-15\n",
      "  4.31307483e-15 4.31307483e-15 4.31307483e-15 1.17144513e-14\n",
      "  1.17229004e-14 7.42050260e-02 9.25794959e-01 5.18401455e-09\n",
      "  6.11493413e-15 4.31307483e-15 4.31307483e-15 4.31307483e-15\n",
      "  4.31465454e-15 4.31376601e-15]\n",
      " [2.14766305e-15 2.14766305e-15 2.14766305e-15 2.14775326e-15\n",
      "  2.14767957e-15 1.25646975e-05 9.99987483e-01 7.02022573e-09\n",
      "  7.63738205e-12 2.69101033e-15 2.14780239e-15 2.14767131e-15\n",
      "  2.14766305e-15 2.14766305e-15 2.14766305e-15 2.14766305e-15\n",
      "  2.14766305e-15 2.14766305e-15 2.14766305e-15 2.14766305e-15\n",
      "  2.14766305e-15 2.14766305e-15 2.14766305e-15 2.99867387e-15\n",
      "  2.14775326e-15 2.14767131e-15 2.14766305e-15 2.14766305e-15\n",
      "  2.14766305e-15 2.14766305e-15]\n",
      " [3.61687846e-14 3.61727860e-14 3.62483447e-14 3.62919262e-14\n",
      "  3.61919052e-14 4.17863376e-14 5.15909709e-14 3.76171194e-14\n",
      "  8.66853315e-14 3.98627068e-13 3.70797888e-14 3.77796210e-14\n",
      "  5.67386002e-14 3.61801009e-14 3.61785831e-14 3.63047334e-14\n",
      "  3.61689946e-14 1.35921647e-11 3.85182969e-10 9.82126415e-01\n",
      "  1.78735666e-02 4.43211169e-13 3.61687846e-14 3.61714748e-14\n",
      "  3.61832045e-14 3.61703058e-14 3.62270571e-14 3.64624069e-14\n",
      "  3.62487580e-14 3.91269557e-14]\n",
      " [4.65075891e-15 4.65077670e-15 4.65075891e-15 4.65081185e-15\n",
      "  4.65100709e-15 4.65100709e-15 4.65287056e-15 4.65892685e-15\n",
      "  4.66474215e-15 4.81902624e-15 4.70622686e-15 4.66725191e-15\n",
      "  4.67468209e-15 1.26500330e-14 1.26108866e-14 1.73184406e-02\n",
      "  9.82681572e-01 5.56444180e-09 3.43592783e-14 4.65148609e-15\n",
      "  4.65091858e-15 4.65255123e-15 4.65100709e-15 4.65081185e-15\n",
      "  4.65129127e-15 4.65356301e-15 4.65445070e-15 4.71048363e-15\n",
      "  4.65361595e-15 4.65148609e-15]\n",
      " [5.49240286e-14 5.49242353e-14 5.49241337e-14 5.49240286e-14\n",
      "  5.53172281e-14 1.67293710e-13 1.51765278e-13 9.74094450e-01\n",
      "  2.59055663e-02 1.58697513e-12 8.92828834e-14 5.51493021e-14\n",
      "  5.49240286e-14 5.49247605e-14 5.49241337e-14 5.49240286e-14\n",
      "  5.49240286e-14 5.49242353e-14 5.49241337e-14 5.49465563e-14\n",
      "  5.49258074e-14 5.49240286e-14 5.49240286e-14 2.22135799e-11\n",
      "  2.24885710e-09 5.49241337e-14 5.49240286e-14 5.49240286e-14\n",
      "  5.49240286e-14 5.49257058e-14]\n",
      " [9.99987483e-01 4.01941690e-19 1.01238299e-19 1.00429762e-19\n",
      "  1.00422098e-19 1.24769567e-05 7.35402339e-19 1.00414828e-19\n",
      "  1.44498087e-19 1.01003761e-19 1.00414440e-19 1.00502192e-19\n",
      "  1.00416741e-19 1.00414440e-19 1.00446235e-19 1.00415590e-19\n",
      "  1.00414440e-19 1.00964464e-19 1.00415209e-19 1.00415590e-19\n",
      "  1.01522546e-19 1.00433207e-19 1.00415590e-19 1.02832173e-19\n",
      "  1.00497591e-19 1.00414440e-19 1.00415978e-19 1.00414440e-19\n",
      "  1.00414440e-19 1.00699837e-19]\n",
      " [1.71144299e-15 1.71144299e-15 1.71144299e-15 1.71144299e-15\n",
      "  1.71144299e-15 1.71454048e-15 1.71163240e-15 1.71145612e-15\n",
      "  1.71191965e-15 1.71300417e-15 1.71151488e-15 1.71149529e-15\n",
      "  1.71145612e-15 1.71144299e-15 1.71144299e-15 1.71144299e-15\n",
      "  1.71144299e-15 1.71144299e-15 1.71144299e-15 1.71144299e-15\n",
      "  1.71144299e-15 1.71144299e-15 1.71144299e-15 3.43719092e-14\n",
      "  9.97529805e-01 5.59445956e-09 2.47020600e-03 1.71144955e-15\n",
      "  1.71144299e-15 1.71144299e-15]\n",
      " [3.00099707e-15 3.00099707e-15 3.00099707e-15 3.00099707e-15\n",
      "  3.00099707e-15 3.00099707e-15 3.00105446e-15 3.00099707e-15\n",
      "  3.00099707e-15 3.00103159e-15 3.00099707e-15 3.00099707e-15\n",
      "  3.00099707e-15 3.00099707e-15 3.00099707e-15 3.00099707e-15\n",
      "  3.00099707e-15 3.00099707e-15 3.00102016e-15 3.00099707e-15\n",
      "  6.01707337e-14 9.95652139e-01 9.81031167e-09 4.34781937e-03\n",
      "  5.27737229e-15 3.00099707e-15 3.00099707e-15 3.00099707e-15\n",
      "  3.00099707e-15 3.00102016e-15]\n",
      " [2.63485035e-15 2.63666025e-15 9.26550984e-01 8.61279670e-09\n",
      "  7.34490007e-02 4.36911241e-15 2.63502124e-15 2.74009314e-15\n",
      "  2.63604679e-15 2.63485035e-15 2.63496110e-15 2.63486031e-15\n",
      "  2.63485035e-15 2.63485035e-15 2.63485035e-15 2.63485035e-15\n",
      "  2.63485035e-15 2.63485035e-15 2.63485035e-15 2.63822980e-15\n",
      "  2.63522241e-15 2.63485035e-15 2.63495073e-15 2.63487047e-15\n",
      "  2.63485035e-15 2.63485035e-15 2.63485035e-15 2.63485035e-15\n",
      "  2.63485035e-15 2.63485035e-15]\n",
      " [8.52827216e-05 9.99914765e-01 6.46765852e-19 9.98609077e-20\n",
      "  9.98643392e-20 9.98906280e-20 9.98609077e-20 9.98616703e-20\n",
      "  9.98612890e-20 9.98605264e-20 9.98605264e-20 9.98609077e-20\n",
      "  9.98605264e-20 9.98605264e-20 9.98605264e-20 9.98605264e-20\n",
      "  9.98609077e-20 9.98612890e-20 9.98605264e-20 9.98609077e-20\n",
      "  9.98609077e-20 9.98605264e-20 9.98605264e-20 9.98609077e-20\n",
      "  9.98605264e-20 9.98605264e-20 9.98605264e-20 9.98605264e-20\n",
      "  9.98605264e-20 9.98605264e-20]\n",
      " [1.28677166e-13 1.28676922e-13 1.28677166e-13 1.28686260e-13\n",
      "  1.28679863e-13 1.28692142e-13 1.30070379e-13 1.29298631e-13\n",
      "  9.51072988e-13 4.94981706e-01 4.17856711e-07 5.05017817e-01\n",
      "  1.91139656e-11 1.28692142e-13 1.28677410e-13 1.28676922e-13\n",
      "  1.28676922e-13 1.28676922e-13 1.28676922e-13 1.28676922e-13\n",
      "  1.28676922e-13 1.28676922e-13 1.28676922e-13 1.28805590e-13\n",
      "  1.29946130e-13 1.28894305e-13 1.28712023e-13 1.28707117e-13\n",
      "  1.28677410e-13 1.28678399e-13]\n",
      " [4.57382333e-15 4.57382333e-15 4.57382333e-15 4.58687569e-15\n",
      "  4.57415495e-15 4.57394573e-15 4.67493535e-15 4.57481817e-15\n",
      "  4.57464368e-15 5.01078857e-15 4.57450435e-15 4.57502739e-15\n",
      "  5.42434394e-15 4.57701749e-15 4.58040605e-15 1.11744881e-14\n",
      "  1.24325387e-14 1.24341980e-14 1.93511117e-02 9.80648875e-01\n",
      "  5.48936940e-09 9.64069438e-15 4.57384112e-15 4.57384112e-15\n",
      "  4.57558601e-15 4.57382333e-15 4.57382333e-15 4.57731438e-15\n",
      "  4.57387585e-15 4.57387585e-15]\n",
      " [1.71093498e-15 1.71093498e-15 1.71093498e-15 1.73812357e-15\n",
      "  1.71097415e-15 1.71107199e-15 1.74114970e-15 1.71096759e-15\n",
      "  1.71111116e-15 1.85363504e-15 1.71100031e-15 1.71095446e-15\n",
      "  1.72819984e-15 1.71094154e-15 1.71093498e-15 1.71093498e-15\n",
      "  1.71093498e-15 1.71093498e-15 1.71093498e-15 1.71093498e-15\n",
      "  1.71093498e-15 1.71093498e-15 1.71093498e-15 1.71111116e-15\n",
      "  4.13289228e-14 9.98181343e-01 5.59141222e-09 1.81862246e-03\n",
      "  1.71093498e-15 1.71093498e-15]\n",
      " [9.75910162e-14 9.75910162e-14 9.75910162e-14 9.75910162e-14\n",
      "  9.75910162e-14 9.75910162e-14 9.75917616e-14 9.75910162e-14\n",
      "  9.75910162e-14 9.75913957e-14 9.75910162e-14 9.75910162e-14\n",
      "  9.75910162e-14 9.75910162e-14 9.75910162e-14 9.75990258e-14\n",
      "  9.75910162e-14 7.21960144e-13 6.16790950e-01 3.19026725e-07\n",
      "  3.83208752e-01 9.00855605e-12 9.75910162e-14 9.75910162e-14\n",
      "  9.75910162e-14 9.75910162e-14 9.75910162e-14 9.75910162e-14\n",
      "  9.75910162e-14 9.75910162e-14]\n",
      " [3.35870891e-14 3.35870891e-14 3.35872823e-14 3.35871569e-14\n",
      "  3.35872823e-14 3.36274113e-14 3.36184937e-14 3.36467846e-14\n",
      "  2.40036012e-11 2.70961475e-10 9.72847342e-01 2.71525998e-02\n",
      "  1.35655922e-11 3.35911278e-14 3.35872823e-14 3.35871569e-14\n",
      "  3.35870891e-14 3.35870891e-14 3.35870891e-14 3.35870891e-14\n",
      "  3.35870891e-14 3.35870891e-14 3.35870891e-14 3.35871569e-14\n",
      "  3.35872823e-14 3.36181109e-14 3.36871102e-14 3.36650162e-14\n",
      "  3.35893321e-14 3.37771870e-14]\n",
      " [1.74644408e-15 1.74644408e-15 1.74644408e-15 1.74687702e-15\n",
      "  1.74645065e-15 1.74645732e-15 1.84446464e-15 1.74674372e-15\n",
      "  1.74645065e-15 1.77184321e-15 1.74645732e-15 1.74644408e-15\n",
      "  1.74722356e-15 1.74644408e-15 1.74644408e-15 1.74729694e-15\n",
      "  1.74644408e-15 1.74644408e-15 1.74664388e-15 1.74644408e-15\n",
      "  1.74644408e-15 1.74647743e-15 1.74644408e-15 1.74644408e-15\n",
      "  1.74732373e-15 1.74647055e-15 1.74655716e-15 9.49271619e-01\n",
      "  5.70802339e-09 5.07283211e-02]\n",
      " [1.27176427e-13 1.27176427e-13 1.27176427e-13 1.27176427e-13\n",
      "  1.27176427e-13 1.27176427e-13 1.27176427e-13 1.27176427e-13\n",
      "  1.27176427e-13 1.27176427e-13 1.27176427e-13 1.27178121e-13\n",
      "  1.27193164e-13 1.27183691e-13 9.40054349e-13 5.00285923e-01\n",
      "  4.15535794e-07 4.99713719e-01 1.88585328e-11 1.27177389e-13\n",
      "  1.27176427e-13 1.27176427e-13 1.27176427e-13 1.27176427e-13\n",
      "  1.27176427e-13 1.27176427e-13 1.27176427e-13 1.27189275e-13\n",
      "  1.27231491e-13 1.27195102e-13]\n",
      " [4.60413271e-15 4.60422038e-15 4.60416744e-15 4.60429068e-15\n",
      "  4.60511612e-15 4.60460663e-15 4.60741793e-15 4.61257043e-15\n",
      "  4.60982647e-15 4.60917552e-15 1.25401322e-14 1.25387210e-14\n",
      "  1.79008730e-02 9.82099175e-01 5.51437385e-09 3.40683089e-14\n",
      "  4.61142736e-15 4.60580137e-15 4.60738278e-15 4.60516906e-15\n",
      "  4.60430805e-15 4.60415008e-15 4.60434320e-15 4.60457190e-15\n",
      "  4.61322138e-15 4.60480017e-15 4.60571328e-15 4.62788648e-15\n",
      "  4.60502845e-15 4.60427290e-15]\n",
      " [3.30200548e-14 3.30200548e-14 3.30200548e-14 3.30200548e-14\n",
      "  3.30200548e-14 3.30200548e-14 3.30201192e-14 3.30200548e-14\n",
      "  3.30201192e-14 3.30202445e-14 3.30201192e-14 3.30206206e-14\n",
      "  3.30266684e-14 3.30215659e-14 1.55417051e-11 5.51324320e-10\n",
      "  9.59084868e-01 4.09151167e-02 6.36012900e-12 3.30209357e-14\n",
      "  3.30281185e-14 3.35569382e-14 3.30212508e-14 3.30203699e-14\n",
      "  3.30202445e-14 3.30200548e-14 3.30200548e-14 3.30201192e-14\n",
      "  3.30203055e-14 3.30250320e-14]\n",
      " [1.27292775e-13 1.27292775e-13 1.27292775e-13 1.27292775e-13\n",
      "  1.27292775e-13 1.27292775e-13 1.27293263e-13 1.27293019e-13\n",
      "  1.27295445e-13 1.27330424e-13 1.27304200e-13 9.40808629e-13\n",
      "  4.99709398e-01 4.15938615e-07 5.00290215e-01 1.89109960e-11\n",
      "  1.27336496e-13 1.27293507e-13 1.27293263e-13 1.27292775e-13\n",
      "  1.27292775e-13 1.27292775e-13 1.27292775e-13 1.27292775e-13\n",
      "  1.27292775e-13 1.27292775e-13 1.27292775e-13 1.27308808e-13\n",
      "  1.27293751e-13 1.27292775e-13]\n",
      " [1.70050970e-15 1.70051626e-15 9.26242862e-03 9.90737617e-01\n",
      "  5.55888979e-09 9.06496185e-15 1.70267778e-15 1.70052918e-15\n",
      "  1.70078880e-15 1.70070430e-15 1.70051626e-15 1.70079515e-15\n",
      "  1.70052918e-15 1.70050970e-15 1.70056814e-15 1.70051626e-15\n",
      "  1.70050970e-15 1.70052272e-15 1.70051626e-15 1.70050970e-15\n",
      "  1.70051626e-15 1.70051626e-15 1.70050970e-15 1.70052918e-15\n",
      "  1.70055512e-15 1.70050970e-15 1.70061357e-15 1.70054210e-15\n",
      "  1.70051626e-15 1.70051626e-15]\n",
      " [3.98952391e-15 3.98952391e-15 3.98953916e-15 4.01091785e-15\n",
      "  3.98996564e-15 4.01168272e-15 3.92710708e-09 3.99511306e-15\n",
      "  3.98967595e-15 4.77578224e-15 3.98952391e-15 3.98952391e-15\n",
      "  3.99442738e-15 3.98952391e-15 3.98952391e-15 3.99810139e-15\n",
      "  3.98952391e-15 3.98952391e-15 3.99145726e-15 3.98952391e-15\n",
      "  3.98952391e-15 4.01954106e-15 3.98952391e-15 3.98953916e-15\n",
      "  4.00416064e-15 3.98952391e-15 1.04433708e-14 1.08446916e-14\n",
      "  1.15716137e-01 8.84283900e-01]\n",
      " [4.22810641e-15 8.95769417e-01 1.37010128e-08 1.04230598e-01\n",
      "  1.13114169e-14 4.19646719e-15 4.24059210e-15 4.19569893e-15\n",
      "  4.19560322e-15 4.20075953e-15 4.19569893e-15 4.19560322e-15\n",
      "  4.20451104e-15 4.19613134e-15 4.19585902e-15 4.37081579e-15\n",
      "  4.20353271e-15 4.19619529e-15 4.33665284e-15 4.19906165e-15\n",
      "  4.19661161e-15 4.38563040e-15 4.20109665e-15 4.19697964e-15\n",
      "  4.20561769e-15 4.19958978e-15 4.19616310e-15 4.28814114e-15\n",
      "  4.19721978e-15 4.19584293e-15]\n",
      " [3.38229641e-14 3.38229641e-14 3.40697827e-14 4.21408344e-14\n",
      "  3.38230928e-14 3.38258034e-14 3.38683481e-14 3.38229641e-14\n",
      "  3.38231572e-14 3.38310922e-14 3.38229641e-14 3.38230285e-14\n",
      "  3.38243837e-14 3.38229641e-14 3.38229641e-14 3.38237366e-14\n",
      "  3.38229641e-14 3.38230285e-14 3.38259287e-14 3.38229641e-14\n",
      "  3.68679358e-11 1.01473558e-08 9.82514441e-01 1.74855553e-02\n",
      "  2.95279480e-13 3.38229641e-14 3.38229641e-14 3.38230928e-14\n",
      "  3.38230285e-14 3.47639499e-14]\n",
      " [1.74510069e-15 1.74297178e-15 1.74267256e-15 1.21687921e-14\n",
      "  1.74332425e-15 1.74338407e-15 1.37481660e-13 1.76230022e-15\n",
      "  1.75906286e-15 1.25395402e-10 1.74812565e-15 1.74512070e-15\n",
      "  5.76198919e-13 1.74388954e-15 1.74313801e-15 3.03524008e-15\n",
      "  1.74271248e-15 1.74257949e-15 1.98954931e-15 1.74254635e-15\n",
      "  1.74249310e-15 1.96474332e-15 1.74252634e-15 1.74251311e-15\n",
      "  2.10554751e-15 1.74257949e-15 1.74255292e-15 1.42218366e-12\n",
      "  1.00000000e+00 5.62675329e-09]\n",
      " [2.25647968e-15 1.92178943e-15 1.96094967e-15 2.66444823e-15\n",
      "  9.87495482e-01 6.20074481e-09 1.25045469e-02 2.20262808e-15\n",
      "  1.90969486e-15 1.96865471e-15 1.90985516e-15 1.90866783e-15\n",
      "  1.90944710e-15 1.90868986e-15 1.90866063e-15 1.90879891e-15\n",
      "  1.90866783e-15 1.90866063e-15 1.90934504e-15 1.91000805e-15\n",
      "  1.90916292e-15 3.55913419e-15 2.39386906e-15 1.91896902e-15\n",
      "  1.95640682e-15 1.95395297e-15 1.90866783e-15 1.90933784e-15\n",
      "  1.90879891e-15 1.90872607e-15]]\n",
      "Test Accuracy: 0.87500\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tflearn\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "\n",
    "from tensorflow.python.platform import gfile\n",
    "from sklearn.utils.extmath import softmax\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "TOKENIZER_RE = re.compile(r\"[A-Z]{2,}(?![a-z])|[A-Z][a-z]+(?=[A-Z])|[\\'\\w\\-]+\", re.UNICODE)\n",
    "MAXLEN = 30\n",
    "\n",
    "SAVE_DIR = \"./save/self-attentive\"\n",
    "SAVE_FILE_PATH = SAVE_DIR + \"/self-attentive.ckpt\"\n",
    "\n",
    "class SelfAttenModel(object):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 batch_size=40, \n",
    "                 vocab_size=200,\n",
    "                 hidden_size=2000,\n",
    "                 label_num=4,\n",
    "                 layer_num=1, \n",
    "                 embedding_size=100, \n",
    "                 keep_prob=0.8, \n",
    "                 max_sequence_length=10,\n",
    "                 num_units=128,\n",
    "                 d_a=350,\n",
    "                 r=30,\n",
    "                 learning_rate=0.01,\n",
    "                 p_coef=0.5,\n",
    "                 use_penalization=True):\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.label_num = label_num\n",
    "        self.layer_num = layer_num\n",
    "        self.embedding_size = embedding_size\n",
    "        self.keep_prob = keep_prob\n",
    "        self.n = self.max_sequence_length = max_sequence_length\n",
    "        self.u = self.num_units = num_units\n",
    "        self.d_a = d_a\n",
    "        self.r = r\n",
    "        self.learning_rate = learning_rate\n",
    "        self.p_coef = p_coef\n",
    "        self.use_penalization = use_penalization\n",
    "        \n",
    "        self._build_placeholder()\n",
    "        self._build_model()\n",
    "        self._build_optimizer()\n",
    "            \n",
    "    def _build_placeholder(self):\n",
    "        self.sources = tf.placeholder(name='sources', shape=[self.batch_size, self.max_sequence_length], dtype=tf.int64)\n",
    "        self.labels = tf.placeholder(name='labels', shape=[self.batch_size], dtype=tf.int64)\n",
    "        self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "    def _build_single_cell(self):\n",
    "        cell = tf.contrib.rnn.BasicLSTMCell(self.num_units)\n",
    "        cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=self.keep_prob)\n",
    "        return cell\n",
    "    \n",
    "    def _build_model(self):\n",
    "        # Word embedding #\n",
    "        with tf.variable_scope(\"embedding\"):\n",
    "            initializer = tf.contrib.layers.xavier_initializer()\n",
    "            embeddings = tf.get_variable(name=\"embedding_encoder\",\n",
    "                                                shape=[self.vocab_size, self.embedding_size], \n",
    "                                                dtype=tf.float32,\n",
    "                                                initializer=initializer,\n",
    "                                                trainable=True)\n",
    "\n",
    "            input_embeddings = tf.nn.embedding_lookup(params=embeddings,\n",
    "                                                      ids=self.sources)\n",
    "\n",
    "        # Bidirectional rnn #\n",
    "        with tf.variable_scope(\"bidirectional_rnn\"):\n",
    "            cell_forward = self._build_single_cell()\n",
    "            cell_backward = self._build_single_cell()\n",
    "            \n",
    "            # outputs is state 'H'\n",
    "            outputs, _ = tf.nn.bidirectional_dynamic_rnn(cell_fw=cell_forward, \n",
    "                                                              cell_bw=cell_backward, \n",
    "                                                              inputs=input_embeddings,\n",
    "                                                              dtype=tf.float32)\n",
    "            \n",
    "            H = tf.concat(outputs, -1)\n",
    "            \n",
    "        # Self Attention #\n",
    "        with tf.variable_scope(\"self_attention\"):\n",
    "            initializer = tf.contrib.layers.xavier_initializer()\n",
    "            W_s1 = tf.get_variable(name=\"W_s1\", shape=[self.d_a, 2*self.u], initializer=initializer)\n",
    "            W_s2 = tf.get_variable(name='W_s2', shape=[self.r, self.d_a],initializer=initializer)\n",
    "            \n",
    "            a_prev = tf.map_fn(lambda x: tf.matmul(W_s1, tf.transpose(x)), H)\n",
    "            a_prev = tf.tanh(a_prev)\n",
    "            a_prev = tf.map_fn(lambda x: tf.matmul(W_s2, x), a_prev)\n",
    "            \n",
    "            self.A = tf.nn.softmax(a_prev)\n",
    "            self.M = tf.matmul(self.A, H)\n",
    "        \n",
    "        # Fully connected layer #\n",
    "        with tf.variable_scope(\"fully_connected_layer\"):\n",
    "            input_fc = tf.layers.flatten(self.M)\n",
    "            layer_fc = tf.contrib.layers.fully_connected(inputs=input_fc, \n",
    "                                                         num_outputs=self.hidden_size,\n",
    "                                                         activation_fn=tf.nn.relu)\n",
    "            \n",
    "            self.logits = tf.contrib.layers.fully_connected(inputs=layer_fc, \n",
    "                                                            num_outputs=self.label_num,\n",
    "                                                            activation_fn=None)\n",
    "            \n",
    "            \n",
    "            \n",
    "    def _build_optimizer(self):\n",
    "        with tf.variable_scope(\"optimizer\"):\n",
    "            cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits,\n",
    "                                                                           labels=self.labels)\n",
    "            self.loss = tf.reduce_mean(cross_entropy)\n",
    "            \n",
    "            if self.use_penalization:\n",
    "                A_T = tf.transpose(self.A, perm=[0, 2, 1])\n",
    "                tile_eye = tf.tile(tf.eye(self.r), [self.batch_size, 1])\n",
    "                tile_eye = tf.reshape(tile_eye, [-1, self.r, self.r])\n",
    "                AA_T = tf.matmul(self.A, A_T) - tile_eye\n",
    "                P = tf.square(tf.norm(AA_T, axis=[-2, -1], ord='fro'))\n",
    "                p_loss = self.p_coef * P\n",
    "                self.loss = self.loss + p_loss\n",
    "            \n",
    "            self.loss = tf.reduce_mean(self.loss)\n",
    "            \n",
    "            params = tf.trainable_variables()\n",
    "            optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "            grad_and_vars = tf.gradients(self.loss, params)\n",
    "            clipped_gradients, _ = tf.clip_by_global_norm(grad_and_vars, 0.5)\n",
    "            self.optimizer = optimizer.apply_gradients(zip(clipped_gradients, params), global_step=self.global_step)\n",
    "            \n",
    "            self.predict = tf.argmax(self.logits, -1)\n",
    "            self.correct_pred = tf.equal(self.predict, self.labels)\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(self.correct_pred, tf.float32))\n",
    "            \n",
    "\n",
    "def load_csv(filepath, target_column=-1, data_column=None, has_header=False):\n",
    "    with gfile.Open(filepath) as csv_file:\n",
    "        data_file = csv.reader(csv_file)\n",
    "        if has_header:\n",
    "            header = next(data_file)\n",
    "\n",
    "        data, target = [], []\n",
    "        for i, row in enumerate(data_file):\n",
    "            data.append([_d for _i, _d in enumerate(row) if _i == data_column])\n",
    "            target.append([int(_d) for _i, _d in enumerate(row) if _i == target_column])\n",
    "\n",
    "        return data, target\n",
    "\n",
    "def token_parse(iterator):\n",
    "    for value in iterator:\n",
    "        return TOKENIZER_RE.findall(value)\n",
    "\n",
    "\n",
    "def string_parser(arr, tokenizer, fit):\n",
    "    if fit == False:\n",
    "        return list(tokenizer.transform(arr)), tokenizer\n",
    "    else:\n",
    "        return list(tokenizer.fit_transform(arr)), tokenizer\n",
    "\n",
    "def main():\n",
    "    # Set mode\n",
    "    is_training = False\n",
    "    \n",
    "    # Preparing data\n",
    "    tokenizer = tflearn.data_utils.VocabularyProcessor(MAXLEN, tokenizer_fn=lambda tokens: [token_parse(x) for x in tokens])\n",
    "    sources_raw, labels = load_csv('./data/ag_news_csv/train.csv', target_column=0, data_column=2)\n",
    "    sources, vocab_processor = string_parser(sources_raw, tokenizer, fit=True)\n",
    "    sources = tflearn.data_utils.pad_sequences(sources, maxlen=MAXLEN)\n",
    "    labels = np.squeeze(labels)\n",
    "        \n",
    "    sources, labels = shuffle(sources, labels)\n",
    "    vocab_size = len(vocab_processor.vocabulary_._mapping)\n",
    "    label_num = int(np.max(labels) + 1)\n",
    "    \n",
    "    # Training options \n",
    "    epoch_nums = 1\n",
    "    batch_size = 80\n",
    "    total = len(sources)\n",
    "    step_nums = int(total/batch_size)\n",
    "    display_step = int(step_nums / 100)\n",
    "     \n",
    "    if is_training == True:\n",
    "        keep_prob = 0.8\n",
    "    else:\n",
    "        keep_prob = 1.0\n",
    "        \n",
    "    model = SelfAttenModel(batch_size=batch_size,\n",
    "                           vocab_size=vocab_size,\n",
    "                           label_num=label_num,\n",
    "                           keep_prob=keep_prob,\n",
    "                           learning_rate=0.01,\n",
    "                           p_coef=0.25,\n",
    "                           max_sequence_length=MAXLEN)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Saver\n",
    "        saver = tf.train.Saver()\n",
    "        ckpt_path = tf.train.latest_checkpoint(checkpoint_dir=SAVE_DIR)\n",
    "        if ckpt_path: \n",
    "            saver.restore(sess, ckpt_path)\n",
    "        else:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "        # train mode\n",
    "        if is_training == True:\n",
    "            for epoch in range(epoch_nums):\n",
    "                print(\"%d Epoch Start\" % epoch)\n",
    "                display_loss = []\n",
    "                display_accuracy = []\n",
    "                for step in range(step_nums):\n",
    "\n",
    "                    batch_start = step * batch_size\n",
    "                    batch_end = batch_start + batch_size\n",
    "                    batch_sources, batch_labels = (sources[batch_start:batch_end], labels[batch_start:batch_end])\n",
    "\n",
    "                    loss, accuracy, predict, _= sess.run([model.loss, model.accuracy, model.predict, model.optimizer], \n",
    "                                                feed_dict={model.sources: batch_sources, \n",
    "                                                           model.labels: batch_labels})\n",
    "                    display_loss.append(loss)\n",
    "                    display_accuracy.append(accuracy)\n",
    "\n",
    "                    if (step % display_step) == 0:\n",
    "                        \n",
    "                        print(\"Step \" + str(step * batch_size) + \", Minibatch Loss= \" + \\\n",
    "                              \"{:.6f}\".format(np.mean(display_loss)) + \", Training Accuracy= \" + \\\n",
    "                              \"{:.5f}\".format(np.mean(display_accuracy)))\n",
    "                        \n",
    "                        display_loss.clear()\n",
    "                        display_accuracy.clear()\n",
    "\n",
    "                        saver.save(sess, SAVE_FILE_PATH)\n",
    "                        \n",
    "                        test_sources_raw, test_labels = load_csv('./data/ag_news_csv/test.csv', target_column=0, data_column=2)\n",
    "                        test_sources, vocab_processor = string_parser(test_sources_raw, tokenizer, fit=True)\n",
    "                        test_sources = tflearn.data_utils.pad_sequences(test_sources, maxlen=MAXLEN)\n",
    "                        test_labels = np.squeeze(test_labels)\n",
    "\n",
    "                        test_total = len(test_sources)\n",
    "                        test_step_nums = int(test_total/batch_size)\n",
    "\n",
    "                        accuracy_list = []\n",
    "                        for step in range(test_step_nums):\n",
    "\n",
    "                            batch_start = step * batch_size\n",
    "                            batch_end = batch_start + batch_size\n",
    "                            batch_sources, batch_labels = (test_sources[batch_start:batch_end], test_labels[batch_start:batch_end])\n",
    "\n",
    "                            A, accuracy = sess.run([model.A, model.accuracy], \n",
    "                                                                feed_dict={model.sources: batch_sources,\n",
    "                                                                           model.labels: batch_labels})\n",
    "                            accuracy_list.append(accuracy)\n",
    "\n",
    "                        print(\"Test Accuracy: {:.5f}\".format(np.mean(accuracy_list)))\n",
    "\n",
    "            print(\"Optimization Finished!\")\n",
    "        \n",
    "        # test mode\n",
    "        else:\n",
    "            \n",
    "            test_sources_raw, test_labels = load_csv('./data/ag_news_csv/test.csv', target_column=0, data_column=2)\n",
    "            test_sources, vocab_processor = string_parser(test_sources_raw, tokenizer, fit=True)\n",
    "            test_sources = tflearn.data_utils.pad_sequences(test_sources, maxlen=MAXLEN)\n",
    "            test_labels = np.squeeze(test_labels)\n",
    "            \n",
    "            test_total = len(test_sources)\n",
    "            test_step_nums = int(test_total/batch_size)\n",
    "    \n",
    "            accuracy_list = []\n",
    "            step = 0 #for step in range(test_step_nums):\n",
    "    \n",
    "            batch_start = step * batch_size\n",
    "            batch_end = batch_start + batch_size\n",
    "            batch_sources, batch_labels = (test_sources[batch_start:batch_end], test_labels[batch_start:batch_end])\n",
    "\n",
    "            A, accuracy = sess.run([model.A, model.accuracy], \n",
    "                                                feed_dict={model.sources: batch_sources,\n",
    "                                                           model.labels: batch_labels})\n",
    "            accuracy_list.append(accuracy)\n",
    "            a_sum = np.sum(A, axis=0)\n",
    "            \n",
    "            print(softmax(a_sum))\n",
    "            print(\"Test Accuracy: {:.5f}\".format(np.mean(accuracy_list)))\n",
    "            \n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
