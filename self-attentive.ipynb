{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class SelfAttenModel(object):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 batch_size=40, \n",
    "                 vocab_size=200,\n",
    "                 hidden_size=2000,\n",
    "                 label_num=4,\n",
    "                 layer_num=2, \n",
    "                 embedding_size=100, \n",
    "                 keep_prob=0.8, \n",
    "                 max_sequence_length=10,\n",
    "                 num_units=128,\n",
    "                 d_a=350,\n",
    "                 r=30,learning_rate=0.01):\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.label_num = label_num\n",
    "        self.layer_num = layer_num\n",
    "        self.embedding_size = embedding_size\n",
    "        self.keep_prob = keep_prob\n",
    "        self.n = self.max_sequence_length = max_sequence_length\n",
    "        self.u = self.num_units = num_units\n",
    "        self.d_a = d_a\n",
    "        self.r = r\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self._build_placeholder()\n",
    "        self._build_model()\n",
    "        self._build_optimizer()\n",
    "            \n",
    "    def _build_placeholder(self):\n",
    "        self.sources = tf.placeholder(name='sources', shape=[self.batch_size, self.max_sequence_length], dtype=tf.int64)\n",
    "        self.labels = tf.placeholder(name='labels', shape=[self.batch_size], dtype=tf.int64)\n",
    "        \n",
    "\n",
    "    def _build_single_cell(self):\n",
    "        cell = tf.contrib.rnn.BasicLSTMCell(self.num_units)\n",
    "        cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=self.keep_prob)\n",
    "        return cell\n",
    "    \n",
    "    def _build_model(self):\n",
    "        # Word embedding #\n",
    "        with tf.variable_scope(\"embedding\"):\n",
    "            initializer = tf.contrib.layers.xavier_initializer()\n",
    "            embeddings = tf.get_variable(name=\"embedding_encoder\",\n",
    "                                                shape=[self.vocab_size, self.embedding_size], \n",
    "                                                dtype=tf.float32,\n",
    "                                                initializer=initializer,\n",
    "                                                trainable=True)\n",
    "\n",
    "            input_embeddings = tf.nn.embedding_lookup(params=embeddings,\n",
    "                                                      ids=self.sources)\n",
    "\n",
    "        # Bidirectional rnn #\n",
    "        with tf.variable_scope(\"bidirectional_rnn\"):\n",
    "            cell_forward = self._build_single_cell()\n",
    "            cell_backward = self._build_single_cell()\n",
    "            \n",
    "            # outputs is state 'H'\n",
    "            outputs, _ = tf.nn.bidirectional_dynamic_rnn(cell_fw=cell_forward, \n",
    "                                                              cell_bw=cell_backward, \n",
    "                                                              inputs=input_embeddings,\n",
    "                                                              dtype=tf.float32)\n",
    "            \n",
    "            H = tf.concat(outputs, -1)\n",
    "            \n",
    "        # Self Attention #\n",
    "        with tf.variable_scope(\"self_attention\"):\n",
    "            initializer = tf.contrib.layers.xavier_initializer()\n",
    "            W_s1 = tf.get_variable(name=\"W_s1\", shape=[self.d_a, 2*self.u], initializer=initializer)\n",
    "            W_s2 = tf.get_variable(name='W_s2', shape=[self.r, self.d_a],initializer=initializer)\n",
    "            \n",
    "            a_prev = tf.map_fn(lambda x: tf.matmul(W_s1, tf.transpose(x)), H)\n",
    "            a_prev = tf.tanh(a_prev)\n",
    "            a_prev = tf.map_fn(lambda x: tf.matmul(W_s2, x), a_prev)\n",
    "            \n",
    "            self.A = tf.nn.softmax(a_prev)\n",
    "            self.M = tf.matmul(self.A, H)\n",
    "        \n",
    "        # Fully connected layer #\n",
    "        with tf.variable_scope(\"fully_connected_layer\"):\n",
    "            input_fc = tf.layers.flatten(self.M)\n",
    "            layer_fc = tf.contrib.layers.fully_connected(inputs=input_fc, \n",
    "                                                         num_outputs=self.hidden_size,\n",
    "                                                         activation_fn=tf.nn.relu)\n",
    "            \n",
    "            self.logits = tf.contrib.layers.fully_connected(inputs=layer_fc, \n",
    "                                                            num_outputs=self.label_num,\n",
    "                                                            activation_fn=None)\n",
    "            \n",
    "            \n",
    "            \n",
    "    def _build_optimizer(self):\n",
    "        with tf.variable_scope(\"optimizer\"):\n",
    "            cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits,\n",
    "                                                                           labels=self.labels)\n",
    "            self.loss = tf.reduce_mean(cross_entropy)\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "            self.optimizer = self.optimizer.minimize(self.loss)\n",
    "\n",
    "            correct_pred = tf.equal(tf.argmax(self.logits, -1), self.labels)\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "            \n",
    "            \n",
    "            \n",
    "def main():\n",
    "    model = SelfAttenModel()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Run the initializer\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        training_steps = 40000\n",
    "        display_step = 200\n",
    "    \n",
    "        for step in range(len(training_steps)):\n",
    "            # Run optimization op (backprop)\n",
    "            sess.run(model.optimizer, feed_dict={model.sources: batch_sources, \n",
    "                                                 model.labels: batch_labels})\n",
    "\n",
    "            if (step % display_step) == 0 or step == 1:\n",
    "                # Calculate batch accuracy & loss\n",
    "                loss, accuracy = sess.run([model.loss, model.accuracy], feed_dict={model.sources: batch_sources, \n",
    "                                                                                   model.labels: batch_labels})\n",
    "\n",
    "                print(\"Step \" + str(step * batch_size) + \", Minibatch Loss= \" + \\\n",
    "                      \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                      \"{:.5f}\".format(accuracy))\n",
    "\n",
    "                break;\n",
    "\n",
    "        print(\"Optimization Finished!\")\n",
    "        print(\"Testing Accuracy:\", sess.run(model.accuracy, feed_dict={model.sources: batch_sources, \n",
    "                                                                       model.labels: batch_labels}))\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
