{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tflearn/data_utils.py:201: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "INFO:tensorflow:Restoring parameters from ./save/self-attentive/self-attentive.ckpt\n",
      "[[6.62786476e-14 6.54452350e-08 5.48238484e-13 8.45891621e-14\n",
      "  2.42867559e-10 1.19928754e-11 5.73222403e-09 2.15298157e-09\n",
      "  1.74321308e-15 7.80633533e-13 3.32030581e-08 2.02302097e-13\n",
      "  3.27016344e-12 1.06622437e-12 4.92470864e-13 3.53026462e-11\n",
      "  9.99940038e-01 4.32734839e-16 4.09034006e-14 8.12606959e-06\n",
      "  1.86109767e-13 4.65263516e-13 4.01367687e-13 1.23182604e-14\n",
      "  1.12036813e-09 5.17867302e-05 7.85982121e-17 1.04216271e-13\n",
      "  5.56332974e-12 1.11006126e-11]\n",
      " [9.20068787e-15 1.78833233e-11 2.46018346e-04 2.13102118e-15\n",
      "  8.24819847e-15 8.51795467e-10 8.13927592e-10 2.96823178e-12\n",
      "  6.10189491e-15 7.29599194e-12 4.96474906e-09 2.23395489e-08\n",
      "  5.03044024e-14 9.95791626e-16 1.57687474e-10 4.25848523e-11\n",
      "  3.70282471e-10 6.83664347e-14 1.60103005e-16 4.18250057e-09\n",
      "  2.59438426e-09 8.81904196e-13 6.27156905e-16 1.78116671e-11\n",
      "  1.16212196e-11 9.41739131e-10 4.58917730e-11 4.85311663e-17\n",
      "  5.89987434e-11 9.99753892e-01]\n",
      " [1.60436061e-16 1.32616096e-08 1.07499667e-01 2.22274639e-18\n",
      "  1.07460228e-10 2.35581413e-01 8.50553482e-12 4.99741633e-13\n",
      "  1.11287271e-10 2.37438514e-13 2.58086893e-05 2.03013662e-02\n",
      "  4.30091567e-17 8.49036286e-16 6.16943479e-01 1.54273657e-05\n",
      "  4.41934978e-09 6.13254170e-11 1.81119434e-17 1.31298220e-05\n",
      "  1.88285913e-02 3.04194122e-16 4.30611512e-19 6.20861319e-05\n",
      "  6.52914278e-10 6.28269659e-09 1.33343214e-09 1.43443296e-20\n",
      "  2.37526630e-11 7.29021500e-04]\n",
      " [5.80987379e-24 6.73724188e-10 8.22510671e-11 1.08395463e-15\n",
      "  5.57106076e-17 6.00732244e-17 7.56202851e-16 1.00000000e+00\n",
      "  1.41043443e-17 3.84502823e-26 8.30571611e-10 2.57462079e-10\n",
      "  1.42942217e-13 3.96553086e-15 1.37291645e-21 8.62950358e-20\n",
      "  3.64946366e-08 1.67618212e-13 5.65270982e-24 2.89471528e-12\n",
      "  3.45080323e-12 2.10227322e-11 3.57708307e-11 1.13649013e-23\n",
      "  9.04015352e-21 1.79974382e-08 1.06957354e-09 4.48564375e-21\n",
      "  2.32647087e-23 2.82690226e-21]\n",
      " [4.09899503e-10 9.11244387e-13 4.74129070e-20 3.60982034e-12\n",
      "  4.68353592e-19 1.15282522e-14 2.52736818e-08 3.77734395e-22\n",
      "  1.43750622e-24 6.15284875e-07 1.75261854e-11 9.91690375e-18\n",
      "  1.41806410e-14 3.82402659e-23 7.90455636e-15 9.99999404e-01\n",
      "  2.44519469e-18 1.64246668e-24 2.12122652e-15 3.47046746e-14\n",
      "  4.75437942e-15 1.85391202e-14 1.59467973e-25 2.09159752e-18\n",
      "  4.02138127e-08 1.95802227e-16 5.50197300e-25 2.10256829e-15\n",
      "  4.29532229e-15 2.28526220e-14]\n",
      " [6.17249343e-25 2.74900708e-22 3.21792787e-10 1.26529816e-14\n",
      "  8.04426241e-14 1.62079199e-16 7.92418360e-26 1.07591713e-18\n",
      "  2.00516777e-04 2.99390435e-22 3.12530510e-22 2.97067134e-16\n",
      "  1.27525045e-15 2.87777380e-09 1.22995667e-12 4.12087684e-24\n",
      "  2.65412868e-25 9.99799311e-01 2.04371872e-07 2.42844050e-25\n",
      "  1.75125769e-16 6.80810073e-19 6.89031419e-14 6.08517503e-09\n",
      "  3.08927841e-25 3.39694722e-26 1.07453928e-10 3.09392182e-12\n",
      "  3.08531231e-23 6.29947479e-24]\n",
      " [6.07714712e-10 1.43607556e-10 1.58613947e-16 1.07675490e-07\n",
      "  9.84676540e-01 5.58645740e-13 2.20368496e-14 1.25367461e-09\n",
      "  4.84575433e-13 9.91860327e-12 7.07131922e-11 1.15607014e-16\n",
      "  1.99780470e-11 4.67375884e-07 6.76814731e-15 3.85156893e-14\n",
      "  5.29369604e-12 3.53603187e-13 1.53228836e-02 4.72826294e-08\n",
      "  1.01145229e-14 6.16862518e-13 2.93441071e-09 5.20301649e-11\n",
      "  2.30936949e-12 2.31894636e-13 2.06400804e-14 7.96132105e-10\n",
      "  3.50446099e-13 3.07559555e-19]\n",
      " [7.84501931e-17 1.32060099e-10 1.30843301e-03 1.18048348e-16\n",
      "  1.09633084e-15 2.71919909e-09 4.86055250e-12 3.18093676e-11\n",
      "  1.52759472e-09 4.61228862e-17 2.79916693e-12 9.98418689e-01\n",
      "  1.01514442e-14 4.50927709e-18 3.50687618e-10 9.14178780e-13\n",
      "  2.70468981e-09 6.10896578e-08 2.20001867e-19 2.46883243e-13\n",
      "  2.67369091e-04 1.42045153e-09 3.92630129e-14 9.26898350e-11\n",
      "  1.47292809e-17 7.66456232e-09 5.38519862e-06 3.46998793e-18\n",
      "  1.13907794e-15 1.22180764e-11]\n",
      " [5.09026932e-10 7.86286109e-05 1.11704916e-11 4.45126271e-12\n",
      "  5.16422391e-01 4.75371820e-07 5.41859580e-13 3.93286086e-07\n",
      "  2.78430473e-10 1.88586509e-05 1.36653965e-04 2.31862897e-11\n",
      "  8.87897798e-15 4.78134825e-05 3.98296033e-05 1.35019947e-11\n",
      "  3.48494666e-07 5.03596408e-15 4.92707541e-10 2.18393162e-01\n",
      "  1.33191430e-10 9.54551041e-14 9.41356006e-08 3.31353846e-07\n",
      "  1.12604918e-10 5.27545535e-06 6.60294200e-15 7.05557973e-11\n",
      "  2.64855713e-01 6.87661594e-11]\n",
      " [9.27479149e-25 2.12184186e-21 1.23906940e-10 2.65245136e-25\n",
      "  2.27022382e-26 7.02439322e-12 1.05150775e-11 1.34454473e-19\n",
      "  1.36952981e-22 1.47220845e-22 3.83290971e-20 5.04893978e-06\n",
      "  1.32608361e-21 2.13530298e-30 2.97613242e-15 7.00033336e-19\n",
      "  8.18195287e-16 5.49345666e-17 5.89106521e-29 6.19502056e-20\n",
      "  9.99994993e-01 5.36285955e-19 6.97062333e-26 1.40193381e-20\n",
      "  3.37061805e-19 1.92481678e-13 1.93303598e-13 1.48450921e-28\n",
      "  3.72910978e-28 9.15696935e-21]\n",
      " [3.63889985e-09 4.36810251e-06 7.74326647e-09 1.31702749e-09\n",
      "  3.13003402e-05 5.87712884e-06 3.16281870e-07 4.65843812e-07\n",
      "  6.38034276e-13 5.67939196e-07 6.34528638e-04 2.28729902e-09\n",
      "  3.67813530e-10 5.12346565e-09 1.32279922e-07 6.76293041e-07\n",
      "  4.02589450e-07 9.42466851e-12 4.81500395e-10 9.99270618e-01\n",
      "  7.15991007e-07 3.16800031e-10 3.49742884e-08 5.34573346e-08\n",
      "  1.09256998e-05 3.89064189e-05 9.39792081e-12 1.70200583e-12\n",
      "  3.67627839e-09 1.70253198e-08]\n",
      " [4.26566665e-04 1.15055582e-06 4.81281495e-06 4.16315634e-05\n",
      "  3.82384790e-09 4.51694848e-03 5.62830508e-01 5.31743005e-09\n",
      "  8.29995850e-10 7.47513041e-05 3.94415247e-06 1.35478470e-03\n",
      "  4.80446441e-04 2.21232711e-12 3.04506939e-05 3.03072900e-01\n",
      "  7.56178852e-05 1.25580417e-08 1.63530459e-07 2.12851937e-05\n",
      "  1.77067006e-03 3.00815739e-02 2.48243808e-12 1.28252582e-08\n",
      "  9.52051878e-02 2.78598662e-07 4.99197995e-06 1.26823689e-08\n",
      "  3.94977953e-08 1.35325513e-06]\n",
      " [4.78958847e-34 8.59817822e-34 4.58480988e-16 4.28806613e-22\n",
      "  5.03521337e-25 1.98317955e-24 4.91767961e-37 1.19222257e-27\n",
      "  9.99986887e-01 5.82136939e-27 0.00000000e+00 4.51617845e-22\n",
      "  2.82472386e-26 6.46718754e-19 1.76192217e-21 0.00000000e+00\n",
      "  1.39169267e-34 1.30599292e-05 3.86585490e-22 2.78744654e-37\n",
      "  7.73728889e-26 3.14317033e-29 2.83190219e-16 2.80221991e-17\n",
      "  0.00000000e+00 4.74379327e-38 2.52134874e-14 3.52172503e-16\n",
      "  1.91610963e-31 5.56095184e-37]\n",
      " [7.93381474e-14 3.98857010e-27 4.41050505e-25 1.90893070e-06\n",
      "  1.33671054e-22 4.48480204e-24 1.27799379e-19 1.05268344e-23\n",
      "  2.14078006e-15 9.39392361e-16 2.80567669e-28 1.63010304e-25\n",
      "  3.28719132e-08 3.31711866e-17 1.64627658e-24 4.10192496e-18\n",
      "  5.13937179e-27 1.97034680e-20 1.08285701e-06 7.38074237e-25\n",
      "  6.81056206e-27 2.01197370e-12 2.59492762e-19 6.03643685e-23\n",
      "  5.46423125e-16 7.10379636e-30 2.85385147e-21 9.99997020e-01\n",
      "  9.71210647e-23 2.60481797e-25]\n",
      " [4.66041509e-15 1.00000000e+00 2.27122567e-16 5.62987860e-16\n",
      "  1.26735489e-09 2.16089513e-16 1.35700708e-17 1.54981095e-08\n",
      "  5.43639952e-19 9.16227466e-17 8.67784689e-09 7.54381109e-18\n",
      "  6.66081605e-19 2.84834413e-11 9.69091305e-19 3.29364874e-18\n",
      "  4.96923724e-10 2.71190647e-19 2.09752758e-18 2.89781754e-10\n",
      "  8.06124833e-19 5.08172495e-18 7.31284061e-11 4.08391972e-18\n",
      "  7.39242421e-19 1.46151269e-09 2.28851052e-20 2.70561230e-20\n",
      "  1.37384150e-13 5.76260906e-19]\n",
      " [8.76989059e-09 4.05183081e-16 1.94867934e-15 1.96508448e-10\n",
      "  4.60466301e-13 6.27573037e-12 8.91574725e-09 2.12565101e-21\n",
      "  3.13791944e-11 9.99999404e-01 9.68308279e-17 2.76398497e-15\n",
      "  6.74748382e-11 1.02276518e-15 6.79144563e-08 3.56473819e-08\n",
      "  2.09437602e-18 2.53145148e-14 1.06381766e-08 5.11009175e-15\n",
      "  4.25489884e-13 3.78793663e-10 1.68550782e-16 1.36275304e-07\n",
      "  3.66229891e-08 1.84852103e-17 8.43714166e-16 1.10872591e-08\n",
      "  3.20073724e-07 9.78021700e-13]\n",
      " [1.40864924e-12 1.55319561e-19 2.48504296e-17 1.37611908e-10\n",
      "  1.38716674e-24 6.69597794e-20 3.35472592e-08 5.17482921e-14\n",
      "  1.04154065e-21 2.99164123e-16 1.71325431e-18 3.54514814e-16\n",
      "  5.21291571e-04 3.17372318e-21 2.71055212e-21 2.02638528e-09\n",
      "  8.70916100e-17 8.79664164e-17 1.47828944e-15 4.74551811e-19\n",
      "  3.12822587e-15 9.99478757e-01 7.49090158e-21 2.28247673e-23\n",
      "  3.17280868e-10 8.55202382e-17 2.37219044e-13 1.66346998e-13\n",
      "  1.77952922e-22 5.22144692e-18]\n",
      " [4.86793317e-09 2.20812390e-18 1.05403082e-19 2.15464270e-13\n",
      "  2.44475096e-16 4.01068406e-15 7.80438880e-10 1.70793700e-22\n",
      "  1.73890467e-18 1.55427138e-09 1.90485203e-17 2.23733253e-19\n",
      "  3.90905626e-16 4.30591401e-19 4.19883179e-16 1.26504607e-09\n",
      "  1.77084914e-18 2.00689223e-21 3.55093711e-12 7.87212702e-14\n",
      "  5.36266154e-19 8.46461253e-16 6.12721329e-23 1.90066591e-15\n",
      "  1.00000000e+00 3.99838056e-18 3.27194008e-22 6.29034534e-15\n",
      "  3.11686576e-14 7.45655650e-16]\n",
      " [9.51666301e-10 7.11011738e-15 8.25457324e-14 8.97047714e-11\n",
      "  5.46364499e-18 4.55808612e-18 9.65835333e-01 1.21003199e-11\n",
      "  2.15456413e-20 8.69089783e-12 2.50364703e-12 1.95317460e-11\n",
      "  1.67311187e-09 7.88428170e-21 8.31306424e-18 8.96621444e-08\n",
      "  1.42102525e-08 1.13630995e-15 1.68126181e-14 1.29997304e-12\n",
      "  8.60207877e-11 2.98244401e-10 9.44524110e-16 3.43401529e-18\n",
      "  1.05468430e-07 3.41644958e-02 2.31311230e-15 5.62289270e-15\n",
      "  3.61800912e-16 1.54441942e-15]\n",
      " [5.65527607e-08 1.00777882e-12 9.99591410e-01 1.59699309e-10\n",
      "  3.84338178e-12 5.24663483e-05 1.44923743e-12 1.97458721e-13\n",
      "  1.92443767e-04 7.11869785e-10 1.01883230e-15 1.56283204e-05\n",
      "  2.17859327e-13 4.29319991e-12 1.68498555e-05 4.11463994e-13\n",
      "  6.61850674e-14 8.32970472e-05 1.46534509e-08 1.07086493e-13\n",
      "  1.60457694e-05 8.51478664e-13 6.48181465e-12 3.01423679e-05\n",
      "  7.65216720e-13 5.28053892e-14 1.70285773e-06 2.53436294e-08\n",
      "  1.38139647e-11 5.26803545e-10]\n",
      " [1.15307453e-24 5.80845167e-17 3.74660125e-09 1.22518858e-21\n",
      "  3.51942571e-20 5.42075770e-13 6.40736657e-22 7.97044201e-12\n",
      "  1.72625664e-11 4.00043537e-27 7.30694494e-17 1.61415645e-08\n",
      "  4.36187822e-17 7.55568106e-16 1.44671044e-16 3.89421462e-24\n",
      "  2.14868982e-14 4.07541194e-07 1.25938198e-22 2.44452344e-18\n",
      "  4.02954319e-12 6.15603942e-19 5.40919780e-14 3.81432138e-17\n",
      "  4.70928882e-23 4.10391169e-16 9.99999523e-01 6.04274206e-23\n",
      "  1.34044830e-27 9.86643990e-21]\n",
      " [1.72009649e-07 4.42998208e-11 1.42214494e-05 9.96869028e-01\n",
      "  4.07299000e-10 2.48881715e-08 8.04659885e-06 5.92654251e-05\n",
      "  2.17792444e-06 7.11251857e-08 7.57813812e-09 3.63684080e-06\n",
      "  1.38897786e-03 1.10130842e-10 5.99516436e-10 5.60671367e-07\n",
      "  3.16837804e-06 7.25380087e-05 1.94659924e-06 1.93369723e-10\n",
      "  1.71795873e-05 1.54077495e-03 6.65498519e-08 1.05404510e-07\n",
      "  2.69029332e-08 5.08781659e-06 1.52981772e-06 3.32704053e-06\n",
      "  5.95007776e-09 8.02491104e-06]\n",
      " [1.28088595e-20 6.83831269e-10 1.39670664e-16 3.13715831e-16\n",
      "  8.55210278e-11 3.75978378e-19 8.13561176e-25 3.59209515e-08\n",
      "  5.55995472e-10 5.14155880e-21 9.92507378e-12 4.72167567e-19\n",
      "  8.63230963e-18 9.99999762e-01 6.51985878e-19 8.94744151e-23\n",
      "  1.61764851e-12 2.25162025e-13 5.03460734e-17 3.78678886e-14\n",
      "  2.25710754e-20 4.74315806e-19 2.77966222e-07 1.25825347e-15\n",
      "  1.53616765e-22 5.20101100e-15 3.28170278e-14 5.04957633e-15\n",
      "  5.63928263e-19 1.86650882e-21]\n",
      " [9.99999762e-01 1.93612182e-10 1.67137772e-16 2.02230694e-07\n",
      "  2.40698828e-14 2.67586242e-15 1.33800104e-09 4.26402781e-15\n",
      "  8.32765471e-17 3.42760403e-10 2.00485132e-11 4.99535690e-15\n",
      "  3.52170226e-09 6.36533878e-16 1.27545745e-15 3.83678955e-09\n",
      "  3.18302486e-13 7.85779809e-18 4.23354754e-12 4.20380154e-11\n",
      "  4.25442938e-13 4.33557856e-09 1.47750286e-16 2.37086487e-14\n",
      "  1.28956925e-08 2.67321386e-12 5.67121712e-17 1.96196445e-11\n",
      "  7.70550683e-14 5.39862697e-13]\n",
      " [3.54335138e-11 2.42054696e-20 7.64309189e-13 2.98146368e-03\n",
      "  5.52648673e-18 1.59491608e-22 5.33149985e-04 1.01095983e-14\n",
      "  4.12829086e-14 2.63246469e-09 5.58450828e-19 4.24364398e-11\n",
      "  9.96360838e-01 1.96265448e-16 6.94679927e-19 5.50098321e-07\n",
      "  8.09684215e-14 7.05083039e-07 4.81328364e-08 1.19564738e-25\n",
      "  1.99642343e-12 1.20416480e-04 3.27017558e-12 4.19865973e-16\n",
      "  4.68422133e-12 4.32675610e-15 1.36628320e-09 2.85806300e-06\n",
      "  8.05750638e-19 4.13890200e-22]\n",
      " [2.66566130e-05 6.88056556e-11 1.21746821e-10 3.07982350e-06\n",
      "  1.42663112e-05 3.02586428e-10 1.57286465e-07 3.08231153e-12\n",
      "  5.09769755e-08 1.42751705e-05 9.11320464e-10 2.19000423e-13\n",
      "  7.68447990e-08 2.06229561e-06 1.72985071e-09 1.64909961e-05\n",
      "  3.18732042e-12 6.10133610e-10 1.18751937e-04 3.56794683e-09\n",
      "  8.43297480e-12 8.13513168e-09 3.29485356e-10 2.09689375e-07\n",
      "  1.90706178e-05 1.12055839e-13 5.19178971e-12 2.83021836e-06\n",
      "  9.99781907e-01 1.15591504e-07]\n",
      " [6.92062033e-21 1.64926127e-22 5.18262072e-18 2.73378248e-24\n",
      "  8.99855169e-13 1.50089628e-11 1.17187987e-25 7.29564941e-28\n",
      "  2.37064762e-13 2.44456609e-13 1.03456966e-20 9.96665267e-19\n",
      "  4.31865571e-28 5.35979381e-19 4.59702642e-06 5.49796726e-24\n",
      "  9.99285312e-25 1.66543767e-17 3.13623914e-21 1.41689297e-18\n",
      "  8.38106279e-17 3.37885390e-29 3.78825219e-22 9.99995351e-01\n",
      "  3.16993815e-23 2.20236968e-24 7.89980232e-20 1.27872177e-21\n",
      "  2.94326907e-14 3.68516039e-20]\n",
      " [3.84114645e-16 2.50522203e-06 6.95117429e-16 4.21048244e-20\n",
      "  6.06025785e-10 8.87194940e-09 4.92460985e-16 2.52200279e-13\n",
      "  1.16821064e-19 8.12469980e-16 9.99984384e-01 1.55247354e-15\n",
      "  3.99259855e-20 2.04496945e-12 3.54952049e-12 2.21526313e-12\n",
      "  3.31343182e-11 1.79118879e-19 1.95898354e-19 1.30876242e-05\n",
      "  9.92941759e-14 5.87814706e-19 7.88552046e-16 5.72209652e-14\n",
      "  6.24499966e-11 2.40455744e-09 5.73510685e-19 6.84500762e-21\n",
      "  5.21872367e-13 1.24342661e-10]\n",
      " [3.25267753e-21 2.70586934e-21 4.21741266e-20 7.83867417e-32\n",
      "  3.48500110e-19 9.99996305e-01 1.49825193e-26 8.63326691e-33\n",
      "  7.00226535e-19 9.83358731e-17 3.33729185e-20 3.63278286e-19\n",
      "  1.08626698e-33 1.05134230e-23 3.70796397e-06 2.55470829e-24\n",
      "  3.90831521e-29 3.80364077e-26 6.14456772e-20 1.18216634e-12\n",
      "  3.78510205e-17 8.56935014e-33 6.16482163e-30 4.45520060e-15\n",
      "  2.07689158e-13 4.13325482e-27 7.04772031e-25 3.47027678e-26\n",
      "  7.88165092e-22 2.82145413e-19]\n",
      " [1.58783922e-33 3.58762770e-14 1.23792728e-18 2.84223378e-22\n",
      "  5.73188339e-15 1.33386441e-26 2.77837812e-27 4.29878000e-09\n",
      "  2.77319315e-17 2.14098123e-31 4.48673087e-17 1.57963412e-19\n",
      "  3.36560601e-22 5.52820550e-12 4.75894362e-27 2.16541605e-32\n",
      "  2.68024110e-11 5.87822672e-12 1.13791560e-26 9.95619708e-22\n",
      "  4.12400628e-21 2.70585306e-25 1.00000000e+00 9.60264855e-23\n",
      "  3.08609822e-37 1.42465694e-12 3.97959008e-17 1.35058815e-24\n",
      "  9.44441763e-27 1.80191071e-31]]\n",
      "0.234375\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tflearn\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from reader import load_csv, VocabDict\n",
    "\n",
    "TOKENIZER_RE = re.compile(r\"[A-Z]{2,}(?![a-z])|[A-Z][a-z]+(?=[A-Z])|[\\'\\w\\-]+\", re.UNICODE)\n",
    "MAXLEN = 30\n",
    "\n",
    "SAVE_DIR = \"./save/self-attentive\"\n",
    "SAVE_FILE_PATH = SAVE_DIR + \"/self-attentive.ckpt\"\n",
    "\n",
    "class SelfAttenModel(object):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 batch_size=40, \n",
    "                 vocab_size=200,\n",
    "                 hidden_size=2000,\n",
    "                 label_num=4,\n",
    "                 layer_num=1, \n",
    "                 embedding_size=100, \n",
    "                 keep_prob=0.8, \n",
    "                 max_sequence_length=10,\n",
    "                 num_units=128,\n",
    "                 d_a=350,\n",
    "                 r=30,\n",
    "                 learning_rate=0.01,\n",
    "                 p_coef=0.5,\n",
    "                 use_penalization=True):\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.label_num = label_num\n",
    "        self.layer_num = layer_num\n",
    "        self.embedding_size = embedding_size\n",
    "        self.keep_prob = keep_prob\n",
    "        self.n = self.max_sequence_length = max_sequence_length\n",
    "        self.u = self.num_units = num_units\n",
    "        self.d_a = d_a\n",
    "        self.r = r\n",
    "        self.learning_rate = learning_rate\n",
    "        self.p_coef = p_coef\n",
    "        self.use_penalization = use_penalization\n",
    "        \n",
    "        self._build_placeholder()\n",
    "        self._build_model()\n",
    "        self._build_optimizer()\n",
    "            \n",
    "    def _build_placeholder(self):\n",
    "        self.sources = tf.placeholder(name='sources', shape=[self.batch_size, self.max_sequence_length], dtype=tf.int64)\n",
    "        self.labels = tf.placeholder(name='labels', shape=[self.batch_size], dtype=tf.int64)\n",
    "        self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "    def _build_single_cell(self):\n",
    "        cell = tf.contrib.rnn.BasicLSTMCell(self.num_units)\n",
    "        cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=self.keep_prob)\n",
    "        return cell\n",
    "    \n",
    "    def _build_model(self):\n",
    "        # Word embedding #\n",
    "        with tf.variable_scope(\"embedding\"):\n",
    "            initializer = tf.contrib.layers.xavier_initializer()\n",
    "            embeddings = tf.get_variable(name=\"embedding_encoder\",\n",
    "                                                shape=[self.vocab_size, self.embedding_size], \n",
    "                                                dtype=tf.float32,\n",
    "                                                initializer=initializer,\n",
    "                                                trainable=True)\n",
    "\n",
    "            input_embeddings = tf.nn.embedding_lookup(params=embeddings,\n",
    "                                                      ids=self.sources)\n",
    "\n",
    "        # Bidirectional rnn #\n",
    "        with tf.variable_scope(\"bidirectional_rnn\"):\n",
    "            cell_forward = self._build_single_cell()\n",
    "            cell_backward = self._build_single_cell()\n",
    "            \n",
    "            # outputs is state 'H'\n",
    "            outputs, _ = tf.nn.bidirectional_dynamic_rnn(cell_fw=cell_forward, \n",
    "                                                              cell_bw=cell_backward, \n",
    "                                                              inputs=input_embeddings,\n",
    "                                                              dtype=tf.float32)\n",
    "            \n",
    "            H = tf.concat(outputs, -1)\n",
    "            \n",
    "        # Self Attention #\n",
    "        with tf.variable_scope(\"self_attention\"):\n",
    "            initializer = tf.contrib.layers.xavier_initializer()\n",
    "            W_s1 = tf.get_variable(name=\"W_s1\", shape=[self.d_a, 2*self.u], initializer=initializer)\n",
    "            W_s2 = tf.get_variable(name='W_s2', shape=[self.r, self.d_a],initializer=initializer)\n",
    "            \n",
    "            a_prev = tf.map_fn(lambda x: tf.matmul(W_s1, tf.transpose(x)), H)\n",
    "            a_prev = tf.tanh(a_prev)\n",
    "            a_prev = tf.map_fn(lambda x: tf.matmul(W_s2, x), a_prev)\n",
    "            \n",
    "            self.A = tf.nn.softmax(a_prev)\n",
    "            self.M = tf.matmul(self.A, H)\n",
    "        \n",
    "        # Fully connected layer #\n",
    "        with tf.variable_scope(\"fully_connected_layer\"):\n",
    "            input_fc = tf.layers.flatten(self.M)\n",
    "            layer_fc = tf.contrib.layers.fully_connected(inputs=input_fc, \n",
    "                                                         num_outputs=self.hidden_size,\n",
    "                                                         activation_fn=tf.nn.relu)\n",
    "            \n",
    "            self.logits = tf.contrib.layers.fully_connected(inputs=layer_fc, \n",
    "                                                            num_outputs=self.label_num,\n",
    "                                                            activation_fn=None)\n",
    "            \n",
    "            \n",
    "            \n",
    "    def _build_optimizer(self):\n",
    "        with tf.variable_scope(\"optimizer\"):\n",
    "            cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits,\n",
    "                                                                           labels=self.labels)\n",
    "            self.loss = tf.reduce_mean(cross_entropy)\n",
    "            \n",
    "            if self.use_penalization:\n",
    "                A_T = tf.transpose(self.A, perm=[0, 2, 1])\n",
    "                tile_eye = tf.tile(tf.eye(self.r), [self.batch_size, 1])\n",
    "                tile_eye = tf.reshape(tile_eye, [-1, self.r, self.r])\n",
    "                AA_T = tf.matmul(self.A, A_T) - tile_eye\n",
    "                P = tf.square(tf.norm(AA_T, axis=[-2, -1], ord='fro'))\n",
    "                p_loss = self.p_coef * P\n",
    "                self.loss = self.loss + p_loss\n",
    "            \n",
    "            self.loss = tf.reduce_mean(self.loss)\n",
    "            \n",
    "            params = tf.trainable_variables()\n",
    "            optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "            grad_and_vars = tf.gradients(self.loss, params)\n",
    "            clipped_gradients, _ = tf.clip_by_global_norm(grad_and_vars, 0.5)\n",
    "            self.optimizer = optimizer.apply_gradients(zip(clipped_gradients, params), global_step=self.global_step)\n",
    "            \n",
    "            self.predict = tf.argmax(self.logits, -1)\n",
    "            self.correct_pred = tf.equal(self.predict, self.labels)\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(self.correct_pred, tf.float32))\n",
    "            \n",
    "            \n",
    "\n",
    "def token_parse(iterator):\n",
    "    for value in iterator:\n",
    "        return TOKENIZER_RE.findall(value)\n",
    "\n",
    "def string_parser(arr, fit):\n",
    "    tokenizer = tflearn.data_utils.VocabularyProcessor(MAXLEN, tokenizer_fn=lambda tokens: [token_parse(x) for x in tokens])\n",
    "    if fit == False:\n",
    "        return list(tokenizer.transform(arr)), tokenizer\n",
    "    else:\n",
    "        return list(tokenizer.fit_transform(arr)), tokenizer\n",
    "\n",
    "def main():\n",
    "    # Set mode\n",
    "    is_training = False\n",
    "    \n",
    "    # Preparing data\n",
    "    label_dict = VocabDict()\n",
    "    sources, labels = load_csv('./data/ag_news_csv/train.csv', target_columns=[0], columns_to_ignore=[1], target_dict=label_dict)\n",
    "    sources, vocab_processor = string_parser(sources, fit=True)\n",
    "    sources = tflearn.data_utils.pad_sequences(sources, maxlen=MAXLEN)\n",
    "    labels = np.argmax(labels, -1)\n",
    "    \n",
    "    sources, labels = shuffle(sources, labels)\n",
    "    vocab_size = len(vocab_processor.vocabulary_._mapping)\n",
    "    label_num = label_dict.size()\n",
    "    \n",
    "    # Training options\n",
    "    batch_size = 128\n",
    "    total = len(sources)\n",
    "    step_nums = int(total/batch_size)\n",
    "    display_step = int(step_nums / 100)\n",
    "\n",
    "    epoch_nums = 1\n",
    "    \n",
    "    model = SelfAttenModel(batch_size=batch_size,\n",
    "                           vocab_size=vocab_size,\n",
    "                           label_num=label_num,\n",
    "                           p_coef=0.25,\n",
    "                           max_sequence_length=MAXLEN)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Saver\n",
    "        saver = tf.train.Saver()\n",
    "        ckpt_path = tf.train.latest_checkpoint(checkpoint_dir=SAVE_DIR)\n",
    "        if ckpt_path: \n",
    "            saver.restore(sess, ckpt_path)\n",
    "        else:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "        # train mode\n",
    "        if is_training == True:\n",
    "            for epoch in range(epoch_nums):\n",
    "                print(\"%d Epoch Start\" % epoch)\n",
    "                display_loss = []\n",
    "                display_accuracy = []\n",
    "                for step in range(step_nums):\n",
    "\n",
    "                    batch_start = step * batch_size\n",
    "                    batch_end = batch_start + batch_size\n",
    "                    batch_sources, batch_labels = (sources[batch_start:batch_end], labels[batch_start:batch_end])\n",
    "\n",
    "                    loss, accuracy, _= sess.run([model.loss, model.accuracy, model.optimizer], \n",
    "                                                feed_dict={model.sources: batch_sources, \n",
    "                                                           model.labels: batch_labels})\n",
    "                    display_loss.append(loss)\n",
    "                    display_accuracy.append(accuracy)\n",
    "\n",
    "                    if (step % display_step) == 0:\n",
    "                        # Calculate batch accuracy & loss\n",
    "                        print(\"Step \" + str(step * batch_size) + \", Minibatch Loss= \" + \\\n",
    "                              \"{:.6f}\".format(np.mean(display_loss)) + \", Training Accuracy= \" + \\\n",
    "                              \"{:.5f}\".format(np.mean(display_accuracy)))\n",
    "\n",
    "                        display_loss.clear()\n",
    "                        display_accuracy.clear()\n",
    "\n",
    "                        saver.save(sess, SAVE_FILE_PATH)\n",
    "\n",
    "            print(\"Optimization Finished!\")\n",
    "        \n",
    "        # test mode\n",
    "        else:\n",
    "            \n",
    "            label_dict = VocabDict()\n",
    "            sources, labels = load_csv('./data/ag_news_csv/test.csv', target_columns=[0], columns_to_ignore=[1], target_dict=label_dict)\n",
    "            sources, vocab_processor = string_parser(sources, fit=True)\n",
    "            sources = tflearn.data_utils.pad_sequences(sources, maxlen=MAXLEN)\n",
    "            labels = np.argmax(labels, -1)\n",
    "            \n",
    "            total = len(sources)\n",
    "            step_nums = int(total/batch_size)\n",
    "    \n",
    "            accuracy_list = []\n",
    "            for step in range(step_nums):\n",
    "                \n",
    "                batch_start = step * batch_size\n",
    "                batch_end = batch_start + batch_size\n",
    "                batch_sources, batch_labels = (sources[batch_start:batch_end], labels[batch_start:batch_end])\n",
    "\n",
    "                A, accuracy = sess.run([model.A, model.accuracy], \n",
    "                                                    feed_dict={model.sources: batch_sources,\n",
    "                                                               model.labels: batch_labels})\n",
    "                accuracy_list.append(accuracy)\n",
    "\n",
    "            print(np.mean(accuracy))\n",
    "            \n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
