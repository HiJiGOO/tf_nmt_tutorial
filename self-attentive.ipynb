{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tflearn/data_utils.py:201: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "INFO:tensorflow:Restoring parameters from ./save/self-attentive/self-attentive.ckpt\n",
      "[[1.99835472e-06 2.72538397e-03 2.99333921e-03 1.60202861e+00\n",
      "  1.64834913e-02 1.55490229e-03 1.53317571e+00 3.21307907e+01\n",
      "  1.39954624e+01 1.90173775e-01 8.50115836e-01 4.24040409e-05\n",
      "  2.56292662e-03 1.80108691e-04 4.10071661e-05 8.31315248e-08\n",
      "  6.71393074e-09 5.78897381e-11 9.99816393e-07 9.22601657e-08\n",
      "  1.58711458e-07 2.12961800e-06 9.99706805e-01 9.99904215e-01\n",
      "  2.76720505e+01 9.94103999e-08 3.14877013e-09 4.10174425e-06\n",
      "  2.30197415e-07 4.97905603e-07]\n",
      " [6.71618150e-10 4.88616836e-09 2.91327524e-06 2.75068169e-06\n",
      "  7.27716042e-07 4.36616247e-05 1.30498738e-05 6.10672669e-06\n",
      "  9.34200885e-04 3.16388527e-04 4.19971417e-04 6.83269930e+00\n",
      "  9.03684616e+00 3.09990788e+01 2.71621647e+01 5.95641708e+00\n",
      "  3.93579947e-04 2.41941004e-03 4.69388394e-03 1.15411931e-04\n",
      "  6.91958121e-05 1.47778997e-06 3.52482571e-10 2.28931057e-07\n",
      "  6.08042058e-07 1.26546837e-07 1.67859765e-03 1.67274440e-03\n",
      "  3.01573141e-06 2.11350425e-06]\n",
      " [2.34691584e-06 6.88700197e-07 6.77710659e-07 1.25009146e-05\n",
      "  4.75440756e-04 4.83857511e-05 7.81991426e-03 1.03346455e+00\n",
      "  1.00895512e+00 2.89472752e+01 3.29514503e+01 1.39902248e+01\n",
      "  2.03173733e+00 1.27625680e-02 3.16232414e-04 1.13335694e-03\n",
      "  2.25344891e-04 7.26109147e-06 1.29774213e-04 2.22024173e-05\n",
      "  2.28564500e-06 6.28434282e-05 5.26223084e-05 6.86900312e-05\n",
      "  2.46768072e-03 1.75267458e-04 7.45212528e-05 9.36039723e-03\n",
      "  1.37009029e-03 3.02922737e-04]\n",
      " [2.81943552e-10 1.49402652e-07 2.01428356e-05 7.69210601e-05\n",
      "  8.28788106e-05 9.66408406e-05 4.76881949e-04 9.58243763e-05\n",
      "  6.42425366e-05 4.68104088e-04 1.67919134e-04 1.33157391e-05\n",
      "  4.22018238e-05 8.28662542e-06 3.42832777e-07 5.47656384e-07\n",
      "  3.55351437e-09 4.22892138e-10 6.71568745e-10 6.73126138e-12\n",
      "  3.46521596e-11 6.55548948e-11 3.19751281e-10 9.99931574e-01\n",
      "  1.00001013e+00 3.09996319e+01 3.29998779e+01 1.39989262e+01\n",
      "  7.40105725e-06 2.54481256e-06]\n",
      " [2.84331598e-08 1.05407016e-05 2.13639373e-06 1.04490612e-02\n",
      "  4.02372360e-04 4.93357191e-04 1.61493957e-01 3.14370845e-05\n",
      "  1.30090812e-05 2.59280088e-03 6.26492010e-07 3.17214926e-07\n",
      "  1.72751999e-04 2.70146061e-09 1.74891934e-09 8.00665305e-07\n",
      "  4.91150809e-09 6.10205975e-09 3.46548376e-07 9.99172390e-01\n",
      "  9.99892116e-01 3.04762020e+01 3.30000229e+01 1.39994402e+01\n",
      "  3.49084198e-01 8.98450452e-08 1.60576850e-08 3.11021580e-07\n",
      "  3.66285385e-04 1.59137853e-04]\n",
      " [8.05496433e-11 7.92242216e-10 1.41262387e-06 4.20027391e-05\n",
      "  6.78275137e-06 2.24897766e+01 3.37743835e+01 1.49999256e+01\n",
      "  8.17643070e+00 2.25537449e-01 6.42660816e-05 2.10049711e-06\n",
      "  4.48024338e-08 4.13966604e-12 1.31264886e-11 6.08630239e-14\n",
      "  3.33502147e-14 5.68010194e-10 1.09149994e-12 2.28136195e-12\n",
      "  3.17285614e-10 3.68532600e-11 7.98866528e-10 3.33788037e-01\n",
      "  4.06496365e-05 3.48330241e-06 1.54626569e-08 5.51875429e-12\n",
      "  1.17309357e-10 4.50290401e-07]\n",
      " [4.28040785e-08 1.10781075e-04 2.19754968e-03 3.39887664e-03\n",
      "  6.39022444e-04 1.44372389e-01 3.55149537e-01 3.92621681e-02\n",
      "  8.74087930e-01 2.39982891e+00 2.48747300e-02 4.35731634e-02\n",
      "  4.50259179e-01 3.13287048e-04 2.70646298e-04 3.75196943e-03\n",
      "  5.79088010e-06 5.92905283e+00 9.27327728e+00 3.09325447e+01\n",
      "  2.69261475e+01 2.50585008e+00 8.72828537e-07 7.39624156e-05\n",
      "  3.97853815e-04 4.11166766e-05 1.61074044e-03 8.08543526e-03\n",
      "  2.20908201e-03 7.86147043e-02]\n",
      " [4.10576906e-09 3.29123031e-06 2.54029089e-07 1.13552114e-05\n",
      "  5.44364411e-05 5.25800060e-05 4.55154077e-04 1.75373314e-03\n",
      "  3.00147780e-03 3.55408080e-02 1.18577331e-02 3.53985955e-03\n",
      "  5.13004838e-03 1.00062931e+00 9.97530818e-01 2.89457626e+01\n",
      "  3.29842758e+01 1.39948778e+01 1.99984193e+00 1.55479094e-04\n",
      "  3.59929400e-05 3.86308529e-04 5.31984442e-05 1.17695499e-05\n",
      "  1.12709880e-04 6.03342312e-04 7.93079496e-04 1.27612054e-02\n",
      "  6.15215627e-04 1.56075082e-04]\n",
      " [3.44588313e-09 3.00224633e-06 1.55653481e-06 1.40402975e-07\n",
      "  7.13330973e-03 1.11379969e+00 1.01638377e+00 3.05065784e+01\n",
      "  2.68795280e+01 3.36363316e+00 4.85859811e-01 4.09361487e-03\n",
      "  4.80753386e-07 1.37308862e-05 2.43349359e-06 1.34042715e-08\n",
      "  8.35706942e-07 3.04504260e-06 1.02945705e-06 4.10996348e-04\n",
      "  3.15931393e-05 2.04838080e-09 5.38274278e-08 6.00250912e+00\n",
      "  1.06199818e+01 1.25233180e-06 8.59577340e-11 1.11278556e-11\n",
      "  5.57516771e-08 3.08121053e-05]\n",
      " [4.37449684e+01 1.38700020e+00 8.17146245e-03 1.54412686e-04\n",
      "  7.48161619e-05 3.24533539e+01 1.99111223e+00 2.94835559e-06\n",
      "  3.63959759e-01 5.85034210e-03 5.42915473e-08 8.71996104e-04\n",
      "  2.28611570e-05 4.10092724e-08 3.17212922e-04 1.20707564e-05\n",
      "  6.71884663e-07 5.46198431e-03 7.14747921e-06 1.19930683e-05\n",
      "  1.09762987e-02 1.86526406e-04 1.09386092e-05 2.37919781e-02\n",
      "  8.28183605e-04 4.81519313e-09 1.58271632e-05 2.13971575e-07\n",
      "  4.41504291e-08 2.83632032e-03]\n",
      " [8.16995041e-16 1.31953122e-16 7.13165832e-07 2.32063741e-07\n",
      "  9.59108704e-09 1.80694216e-03 1.12074165e-04 6.46046374e-06\n",
      "  2.77662301e-04 9.12155141e-04 4.37275776e-05 2.92010500e-05\n",
      "  5.76080583e-06 3.84766490e-11 1.98136998e-13 9.58531028e-15\n",
      "  6.40239604e-19 3.57035008e-17 1.42549176e-17 1.29650500e-18\n",
      "  1.21128151e-13 4.43455689e-12 2.67881620e-11 2.99990273e+00\n",
      "  3.39989662e+01 1.49999514e+01 2.79979858e+01 2.43847740e-06\n",
      "  6.72636556e-12 1.44198759e-11]\n",
      " [5.86381692e-13 1.11016841e-19 6.36360702e-13 7.06917490e-07\n",
      "  8.69057633e-14 1.20327248e-09 1.88045069e-05 1.03499864e-13\n",
      "  6.86051993e-10 1.05073632e-05 3.22622647e-16 2.17010559e-11\n",
      "  1.65615995e-06 5.31224786e-17 5.18876392e-12 1.44119284e-07\n",
      "  1.40262396e-14 2.07636658e-10 8.63329024e-06 8.24776513e-14\n",
      "  2.99824071e+00 3.34354744e+01 1.50000000e+01 2.80017509e+01\n",
      "  5.64484239e-01 2.22083297e-13 9.61949230e-14 9.06788600e-09\n",
      "  2.76344787e-14 7.81750714e-06]\n",
      " [3.19659641e-08 6.88547385e-04 3.34936638e+01 1.49999332e+01\n",
      "  3.09587860e+01 5.05734503e-01 6.59824727e-05 3.91650088e-02\n",
      "  4.53290180e-04 2.73545993e-07 4.17301962e-05 4.64445884e-06\n",
      "  3.41925221e-09 1.00346348e-07 1.96212806e-08 7.62453035e-12\n",
      "  5.74507286e-09 1.66758056e-08 1.31213858e-08 1.28251698e-03\n",
      "  1.40221367e-04 3.99925177e-08 3.87548680e-05 7.93345771e-06\n",
      "  7.94613619e-09 8.72199823e-09 1.08089360e-09 1.11862325e-11\n",
      "  1.53921675e-07 4.46435621e-07]\n",
      " [3.43809738e+01 4.37504272e+01 1.86820924e+00 2.73243359e-06\n",
      "  3.67542016e-05 3.00104963e-04 2.11826227e-06 1.23544341e-05\n",
      "  8.61368790e-06 5.60757144e-07 1.84273165e-06 2.32695766e-06\n",
      "  1.31646914e-07 7.14791895e-07 1.80301038e-06 2.94916703e-07\n",
      "  3.29899694e-06 6.43439989e-06 8.71349869e-07 1.91255981e-06\n",
      "  4.12556165e-06 2.69846197e-07 1.56114527e-06 1.93661299e-06\n",
      "  4.17349895e-07 1.81718838e-06 5.81515621e-07 3.50396547e-08\n",
      "  1.61021461e-07 3.87546294e-07]\n",
      " [1.24802875e-06 3.12683596e-07 1.06750508e-06 7.24858546e-05\n",
      "  2.32874081e-05 1.18785450e-04 1.07700303e-02 4.81972564e-03\n",
      "  2.00028658e+00 2.89782372e+01 1.49933443e+01 2.89983101e+01\n",
      "  5.00086975e+00 1.18293407e-04 3.44004343e-06 4.36912273e-07\n",
      "  3.94052790e-09 2.11906284e-10 2.97776026e-09 1.75171344e-09\n",
      "  1.84448066e-08 3.18406364e-07 8.33176159e-07 9.99023789e-04\n",
      "  9.81453247e-03 1.68891624e-03 2.72576057e-04 2.33903294e-04\n",
      "  4.33025707e-06 1.13026126e-05]\n",
      " [5.29005142e-08 9.37076663e-07 3.43697110e-07 2.84851459e-03\n",
      "  7.06467108e-05 2.77373874e-05 2.18668524e-02 2.16060987e-04\n",
      "  1.79414477e-04 9.12454054e-02 1.48136096e-04 2.64236442e-04\n",
      "  1.70547426e-01 6.98761491e-04 1.43825240e-03 8.93283308e-01\n",
      "  9.99967396e-01 1.00010061e+00 2.90734215e+01 3.29988861e+01\n",
      "  1.39979753e+01 7.45644450e-01 1.95378584e-06 4.09078211e-06\n",
      "  3.84125713e-04 3.55423992e-08 5.08657898e-08 7.61467556e-04\n",
      "  1.07068345e-05 1.05263553e-05]\n",
      " [5.25092636e-09 2.59902635e-07 1.30846971e-07 1.57643836e-02\n",
      "  2.24392516e-05 8.14630548e-05 1.75058693e-02 1.97932713e-05\n",
      "  1.02176302e-04 8.01091865e-02 3.67963657e-05 1.13942069e-05\n",
      "  1.00393221e-02 3.95011102e-06 7.28318170e-08 5.92340768e-07\n",
      "  2.21061212e-12 2.21431445e-12 1.98560191e-09 6.06757374e-13\n",
      "  3.94066420e-12 3.84575172e-09 1.38237226e-06 1.03037455e-04\n",
      "  3.18452239e+00 3.39999161e+01 1.49997015e+01 2.76920605e+01\n",
      "  1.61915850e-07 6.34268318e-08]\n",
      " [3.16219761e-14 7.52754961e-15 1.60058422e-13 4.20118150e-12\n",
      "  2.43303305e-16 1.78576265e-08 7.67836354e-06 2.80897354e-11\n",
      "  3.55788004e-08 3.06931497e-06 2.28401716e-13 6.31625086e-10\n",
      "  5.10887332e-08 6.51818573e-12 2.32911816e-07 8.16083848e-05\n",
      "  2.59666000e-09 2.00118399e+00 2.94747658e+01 1.50000000e+01\n",
      "  2.89988155e+01 4.52514458e+00 2.49877252e-09 1.92507191e-10\n",
      "  1.27457794e-10 1.48155316e-15 3.90666266e-14 2.74005593e-11\n",
      "  7.89080953e-12 2.02404365e-07]\n",
      " [3.01270253e-09 4.59581486e-07 4.89959803e-06 1.10711778e-06\n",
      "  5.54258031e-06 1.19901996e-03 9.34745301e-04 1.77660503e-03\n",
      "  6.57181740e+00 8.99559021e+00 3.09971066e+01 2.74183521e+01\n",
      "  6.00115013e+00 1.19950557e-04 6.30908926e-06 9.77458171e-07\n",
      "  1.90817474e-07 1.84628504e-07 1.68742673e-07 7.64031086e-07\n",
      "  1.86250134e-07 2.97497369e-08 1.07851605e-09 2.40141753e-06\n",
      "  5.29399358e-06 9.22763837e-04 2.97310785e-03 2.31742626e-03\n",
      "  6.66269916e-05 5.64426510e-03]\n",
      " [1.05958550e-13 3.78454363e-15 4.65536365e-09 2.47828255e-04\n",
      "  2.56264116e-06 6.24015274e-06 5.46091869e-02 1.73019274e-04\n",
      "  5.65226355e-06 1.44381328e-02 5.81525956e-06 2.10510876e-07\n",
      "  4.45265236e-04 7.77293337e-07 9.36682483e-08 4.89351281e-04\n",
      "  7.51086191e-07 5.95788094e-08 1.15391544e-04 3.84571024e-08\n",
      "  1.10694853e-09 1.72315795e-05 4.53165150e-09 3.54137235e-08\n",
      "  5.04004362e-04 1.56274509e-05 6.60415753e-05 3.39291344e+01\n",
      "  1.49998016e+01 3.09999237e+01]\n",
      " [1.26774818e-15 1.78397548e-15 8.81172739e-13 2.35093520e-13\n",
      "  5.79587512e-14 5.98642594e-11 2.36462849e-09 1.01494146e-09\n",
      "  3.15836530e-08 3.07422994e-07 8.65723564e-08 1.28567572e-05\n",
      "  1.30981920e-04 5.68630057e-05 2.00036287e+00 2.90006256e+01\n",
      "  1.49995041e+01 2.89994812e+01 4.99914598e+00 7.43603641e-06\n",
      "  3.87932829e-07 6.12831599e-08 4.38589304e-10 2.31681077e-10\n",
      "  1.90249372e-10 4.07143451e-11 8.01302669e-09 1.00299825e-04\n",
      "  4.32438334e-04 1.46840219e-04]\n",
      " [2.38303901e-08 1.94687746e-05 8.68522511e-06 3.51840790e-05\n",
      "  2.15061853e-04 1.04053375e-04 7.14914640e-04 1.83131965e-03\n",
      "  1.23714353e-03 1.09530357e-03 1.00198078e+00 1.00186682e+00\n",
      "  2.89889164e+01 3.29937592e+01 1.39959154e+01 2.00141358e+00\n",
      "  1.58270134e-03 3.63143539e-04 7.04963692e-04 2.25130410e-04\n",
      "  3.85349740e-05 3.13813189e-06 4.73669315e-05 9.35014032e-05\n",
      "  1.97400292e-03 1.46198799e-04 3.42878368e-04 5.14412113e-03\n",
      "  1.95700282e-04 3.10496616e-05]\n",
      " [2.32322606e-08 1.21379855e-07 2.33146640e-08 4.69461611e-07\n",
      "  1.28235591e-08 1.64264819e-07 1.18578885e-06 6.62932038e-08\n",
      "  1.44675528e-06 4.80385415e-06 2.38987468e-06 1.79182116e-05\n",
      "  1.99583752e-04 4.58306022e-05 6.15416718e+00 9.72296333e+00\n",
      "  3.09998856e+01 2.78454056e+01 5.26068830e+00 2.70109376e-05\n",
      "  2.44799012e-04 1.61289237e-02 3.56031996e-05 1.02530166e-05\n",
      "  6.34173693e-06 2.76570722e-09 1.30591829e-07 1.24192843e-06\n",
      "  7.37807250e-06 1.49923741e-04]\n",
      " [1.41947801e-13 2.76749533e-14 8.17383401e-12 2.35058722e-10\n",
      "  5.93160243e-11 1.43648762e-07 4.57250098e-06 2.20976267e-06\n",
      "  2.08216352e-05 2.96505634e-04 8.87851274e-05 2.00024939e+00\n",
      "  2.89985580e+01 1.49995584e+01 2.89997196e+01 5.00100994e+00\n",
      "  3.42439220e-04 6.23194865e-06 2.97830275e-06 1.65735585e-07\n",
      "  7.97703181e-10 1.09568379e-11 1.03096013e-11 2.63956745e-09\n",
      "  8.61461089e-08 1.07623990e-08 3.40717520e-07 1.25777398e-04\n",
      "  7.64779725e-06 1.92462135e-08]\n",
      " [7.26273157e-08 1.92224616e-06 2.93260593e+01 3.39985428e+01\n",
      "  1.49999809e+01 1.67348838e+00 1.27222284e-03 9.55846826e-06\n",
      "  1.62759316e-04 1.12654227e-04 2.93343578e-06 1.66992235e-04\n",
      "  1.20466766e-05 2.57302815e-07 3.41715167e-05 2.92829145e-06\n",
      "  1.94686763e-07 9.24262531e-06 5.38581753e-06 1.08155596e-07\n",
      "  2.38508210e-06 5.19783907e-06 1.55375972e-07 1.03133716e-05\n",
      "  2.79507985e-05 2.76617527e-07 6.18032864e-05 1.99708702e-05\n",
      "  4.32699971e-06 4.51670576e-06]\n",
      " [2.18052554e-09 4.36455650e-07 2.32197181e-06 5.34903537e-03\n",
      "  1.12151371e-04 5.53766219e-03 1.37997417e+01 1.40081299e-03\n",
      "  3.97268232e-05 1.79884285e-01 1.94816138e-07 2.47167293e-07\n",
      "  1.22957677e-03 4.46836054e-07 4.74729546e-07 2.14605010e-03\n",
      "  9.86423032e-09 8.75830963e-08 4.85620287e-04 5.08455962e-08\n",
      "  1.28775741e-06 7.49639608e-03 7.54091971e-08 2.12918212e-06\n",
      "  3.66133824e-03 1.34095988e-08 9.62293863e-01 1.00000405e+00\n",
      "  3.09984894e+01 3.30321274e+01]\n",
      " [7.71915074e-03 3.29946709e+01 1.49989481e+01 3.08435936e+01\n",
      "  9.91780758e-01 2.11037695e-04 1.06692407e-02 2.69944921e-05\n",
      "  4.86502040e-06 1.23135536e-03 2.51874353e-05 4.51896221e-06\n",
      "  2.12485367e-03 1.30321641e-04 6.30927461e-05 4.09165323e-02\n",
      "  1.89312536e-03 1.44484933e-04 3.30707692e-02 8.27546581e-04\n",
      "  2.44159601e-04 4.43017296e-02 1.31132698e-03 3.31288320e-04\n",
      "  2.38887593e-03 9.54784220e-04 1.35806913e-04 2.18209475e-02\n",
      "  3.88975750e-04 6.25185421e-05]\n",
      " [1.15922800e-08 4.52753426e-08 7.27111753e-03 2.19877347e-01\n",
      "  3.94096742e-06 8.40306529e-05 1.34114840e-03 1.48306958e-08\n",
      "  6.61960303e-06 2.40425419e-04 1.07135190e-09 1.03224852e-06\n",
      "  4.16334551e-05 9.43744083e-10 4.35451938e-07 2.28601511e-05\n",
      "  1.90798888e-09 2.16047306e-06 8.83994580e-05 2.95088221e-08\n",
      "  6.99395704e+00 1.26115828e+01 3.09999962e+01 2.69712563e+01\n",
      "  2.16678238e+00 2.80016510e-10 3.14502095e-08 3.25615929e-06\n",
      "  1.28988472e-06 2.74406839e-02]\n",
      " [1.51943089e-03 2.97042192e-04 1.24767394e-04 1.94358218e+00\n",
      "  4.99522430e-04 5.33156679e-04 4.36819601e+00 1.13259694e-02\n",
      "  9.48539097e-03 1.11839323e+01 3.25102801e-03 1.52860046e-03\n",
      "  5.80115891e+00 8.24866409e-04 3.94226285e-04 5.54996371e-01\n",
      "  1.47619663e-04 7.26901490e-05 1.32614151e-01 5.23000381e-05\n",
      "  2.30700552e-05 1.20066136e-01 4.04050297e-05 3.36458725e-05\n",
      "  1.89279944e-01 7.32001790e-05 5.69411168e-05 6.70465422e+00\n",
      "  3.39834824e+01 1.49877491e+01]\n",
      " [1.67405546e-01 6.85508829e-03 2.70269867e-02 3.33597183e-01\n",
      "  3.38797913e+01 1.49937792e+01 2.95107117e+01 1.43248290e-01\n",
      "  5.40519541e-04 3.09473444e-02 6.24487468e-04 3.66666404e-06\n",
      "  4.11702524e-04 1.54644604e-05 4.36840281e-07 7.23593548e-05\n",
      "  2.00484715e-06 2.78578312e-07 3.59230442e-04 7.06022955e-04\n",
      "  2.64995702e-04 6.23115182e-01 2.26508722e-01 5.38818818e-03\n",
      "  2.47092117e-02 2.34526284e-02 3.68298356e-06 3.55386641e-04\n",
      "  7.29676030e-05 3.25383953e-05]]\n",
      "[[1.09844325e-14 1.10143497e-14 1.10173338e-14 5.45164872e-14\n",
      "  1.11669512e-14 1.10015002e-14 5.08892516e-14 9.88555610e-01\n",
      "  1.31500704e-08 1.32851841e-14 2.57025965e-14 1.09848527e-14\n",
      "  1.10125862e-14 1.09863612e-14 1.09848527e-14 1.09843910e-14\n",
      "  1.09843910e-14 1.09843910e-14 1.09843910e-14 1.09843910e-14\n",
      "  1.09843910e-14 1.09844325e-14 2.98499018e-14 2.98558243e-14\n",
      "  1.14444476e-02 1.09843910e-14 1.09843910e-14 1.09844325e-14\n",
      "  1.09843910e-14 1.09843910e-14]\n",
      " [3.37292958e-14 3.37292958e-14 3.37294212e-14 3.37293568e-14\n",
      "  3.37292958e-14 3.37307730e-14 3.37297430e-14 3.37294855e-14\n",
      "  3.37608325e-14 3.37399752e-14 3.37434514e-14 3.12903661e-11\n",
      "  2.83569612e-10 9.78894949e-01 2.11050063e-02 1.30270577e-11\n",
      "  3.37425468e-14 3.38109667e-14 3.38879925e-14 3.37332193e-14\n",
      "  3.37316099e-14 3.37293568e-14 3.37292958e-14 3.37292958e-14\n",
      "  3.37292958e-14 3.37292958e-14 3.37859555e-14 3.37857590e-14\n",
      "  3.37294212e-14 3.37293568e-14]\n",
      " [4.80306772e-15 4.80304908e-15 4.80304908e-15 4.80310456e-15\n",
      "  4.80534031e-15 4.80328752e-15 4.84075687e-15 1.35003482e-14\n",
      "  1.31734841e-14 1.79126114e-02 9.82087314e-01 5.71997960e-09\n",
      "  3.66344596e-14 4.86474823e-15 4.80457035e-15 4.80849423e-15\n",
      "  4.80413074e-15 4.80308593e-15 4.80367250e-15 4.80315962e-15\n",
      "  4.80306772e-15 4.80334258e-15 4.80330573e-15 4.80337900e-15\n",
      "  4.81491855e-15 4.80389230e-15 4.80341585e-15 4.84822304e-15\n",
      "  4.80963180e-15 4.80449708e-15]\n",
      " [4.10415500e-15 4.10415500e-15 4.10423335e-15 4.10446798e-15\n",
      "  4.10449932e-15 4.10454633e-15 4.10611207e-15 4.10454633e-15\n",
      "  4.10442097e-15 4.10608073e-15 4.10484364e-15 4.10420201e-15\n",
      "  4.10432695e-15 4.10418634e-15 4.10415500e-15 4.10415500e-15\n",
      "  4.10415500e-15 4.10415500e-15 4.10415500e-15 4.10415500e-15\n",
      "  4.10415500e-15 4.10415500e-15 4.10415500e-15 1.11554832e-14\n",
      "  1.11563557e-14 1.19177096e-01 8.80822957e-01 4.93037700e-09\n",
      "  4.10418634e-15 4.10417067e-15]\n",
      " [4.31307483e-15 4.31312395e-15 4.31309134e-15 4.35837584e-15\n",
      "  4.31480277e-15 4.31519791e-15 5.06901362e-15 4.31320654e-15\n",
      "  4.31312395e-15 4.32427726e-15 4.31307483e-15 4.31307483e-15\n",
      "  4.31381513e-15 4.31307483e-15 4.31307483e-15 4.31307483e-15\n",
      "  4.31307483e-15 4.31307483e-15 4.31307483e-15 1.17144513e-14\n",
      "  1.17229004e-14 7.42050260e-02 9.25794959e-01 5.18401455e-09\n",
      "  6.11493413e-15 4.31307483e-15 4.31307483e-15 4.31307483e-15\n",
      "  4.31465454e-15 4.31376601e-15]\n",
      " [2.14766305e-15 2.14766305e-15 2.14766305e-15 2.14775326e-15\n",
      "  2.14767957e-15 1.25646975e-05 9.99987483e-01 7.02022573e-09\n",
      "  7.63738205e-12 2.69101033e-15 2.14780239e-15 2.14767131e-15\n",
      "  2.14766305e-15 2.14766305e-15 2.14766305e-15 2.14766305e-15\n",
      "  2.14766305e-15 2.14766305e-15 2.14766305e-15 2.14766305e-15\n",
      "  2.14766305e-15 2.14766305e-15 2.14766305e-15 2.99867387e-15\n",
      "  2.14775326e-15 2.14767131e-15 2.14766305e-15 2.14766305e-15\n",
      "  2.14766305e-15 2.14766305e-15]\n",
      " [3.61687846e-14 3.61727860e-14 3.62483447e-14 3.62919262e-14\n",
      "  3.61919052e-14 4.17863376e-14 5.15909709e-14 3.76171194e-14\n",
      "  8.66853315e-14 3.98627068e-13 3.70797888e-14 3.77796210e-14\n",
      "  5.67386002e-14 3.61801009e-14 3.61785831e-14 3.63047334e-14\n",
      "  3.61689946e-14 1.35921647e-11 3.85182969e-10 9.82126415e-01\n",
      "  1.78735666e-02 4.43211169e-13 3.61687846e-14 3.61714748e-14\n",
      "  3.61832045e-14 3.61703058e-14 3.62270571e-14 3.64624069e-14\n",
      "  3.62487580e-14 3.91269557e-14]\n",
      " [4.65075891e-15 4.65077670e-15 4.65075891e-15 4.65081185e-15\n",
      "  4.65100709e-15 4.65100709e-15 4.65287056e-15 4.65892685e-15\n",
      "  4.66474215e-15 4.81902624e-15 4.70622686e-15 4.66725191e-15\n",
      "  4.67468209e-15 1.26500330e-14 1.26108866e-14 1.73184406e-02\n",
      "  9.82681572e-01 5.56444180e-09 3.43592783e-14 4.65148609e-15\n",
      "  4.65091858e-15 4.65255123e-15 4.65100709e-15 4.65081185e-15\n",
      "  4.65129127e-15 4.65356301e-15 4.65445070e-15 4.71048363e-15\n",
      "  4.65361595e-15 4.65148609e-15]\n",
      " [5.49240286e-14 5.49242353e-14 5.49241337e-14 5.49240286e-14\n",
      "  5.53172281e-14 1.67293710e-13 1.51765278e-13 9.74094450e-01\n",
      "  2.59055663e-02 1.58697513e-12 8.92828834e-14 5.51493021e-14\n",
      "  5.49240286e-14 5.49247605e-14 5.49241337e-14 5.49240286e-14\n",
      "  5.49240286e-14 5.49242353e-14 5.49241337e-14 5.49465563e-14\n",
      "  5.49258074e-14 5.49240286e-14 5.49240286e-14 2.22135799e-11\n",
      "  2.24885710e-09 5.49241337e-14 5.49240286e-14 5.49240286e-14\n",
      "  5.49240286e-14 5.49257058e-14]\n",
      " [9.99987483e-01 4.01941690e-19 1.01238299e-19 1.00429762e-19\n",
      "  1.00422098e-19 1.24769567e-05 7.35402339e-19 1.00414828e-19\n",
      "  1.44498087e-19 1.01003761e-19 1.00414440e-19 1.00502192e-19\n",
      "  1.00416741e-19 1.00414440e-19 1.00446235e-19 1.00415590e-19\n",
      "  1.00414440e-19 1.00964464e-19 1.00415209e-19 1.00415590e-19\n",
      "  1.01522546e-19 1.00433207e-19 1.00415590e-19 1.02832173e-19\n",
      "  1.00497591e-19 1.00414440e-19 1.00415978e-19 1.00414440e-19\n",
      "  1.00414440e-19 1.00699837e-19]\n",
      " [1.71144299e-15 1.71144299e-15 1.71144299e-15 1.71144299e-15\n",
      "  1.71144299e-15 1.71454048e-15 1.71163240e-15 1.71145612e-15\n",
      "  1.71191965e-15 1.71300417e-15 1.71151488e-15 1.71149529e-15\n",
      "  1.71145612e-15 1.71144299e-15 1.71144299e-15 1.71144299e-15\n",
      "  1.71144299e-15 1.71144299e-15 1.71144299e-15 1.71144299e-15\n",
      "  1.71144299e-15 1.71144299e-15 1.71144299e-15 3.43719092e-14\n",
      "  9.97529805e-01 5.59445956e-09 2.47020600e-03 1.71144955e-15\n",
      "  1.71144299e-15 1.71144299e-15]\n",
      " [3.00099707e-15 3.00099707e-15 3.00099707e-15 3.00099707e-15\n",
      "  3.00099707e-15 3.00099707e-15 3.00105446e-15 3.00099707e-15\n",
      "  3.00099707e-15 3.00103159e-15 3.00099707e-15 3.00099707e-15\n",
      "  3.00099707e-15 3.00099707e-15 3.00099707e-15 3.00099707e-15\n",
      "  3.00099707e-15 3.00099707e-15 3.00102016e-15 3.00099707e-15\n",
      "  6.01707337e-14 9.95652139e-01 9.81031167e-09 4.34781937e-03\n",
      "  5.27737229e-15 3.00099707e-15 3.00099707e-15 3.00099707e-15\n",
      "  3.00099707e-15 3.00102016e-15]\n",
      " [2.63485035e-15 2.63666025e-15 9.26550984e-01 8.61279670e-09\n",
      "  7.34490007e-02 4.36911241e-15 2.63502124e-15 2.74009314e-15\n",
      "  2.63604679e-15 2.63485035e-15 2.63496110e-15 2.63486031e-15\n",
      "  2.63485035e-15 2.63485035e-15 2.63485035e-15 2.63485035e-15\n",
      "  2.63485035e-15 2.63485035e-15 2.63485035e-15 2.63822980e-15\n",
      "  2.63522241e-15 2.63485035e-15 2.63495073e-15 2.63487047e-15\n",
      "  2.63485035e-15 2.63485035e-15 2.63485035e-15 2.63485035e-15\n",
      "  2.63485035e-15 2.63485035e-15]\n",
      " [8.52827216e-05 9.99914765e-01 6.46765852e-19 9.98609077e-20\n",
      "  9.98643392e-20 9.98906280e-20 9.98609077e-20 9.98616703e-20\n",
      "  9.98612890e-20 9.98605264e-20 9.98605264e-20 9.98609077e-20\n",
      "  9.98605264e-20 9.98605264e-20 9.98605264e-20 9.98605264e-20\n",
      "  9.98609077e-20 9.98612890e-20 9.98605264e-20 9.98609077e-20\n",
      "  9.98609077e-20 9.98605264e-20 9.98605264e-20 9.98609077e-20\n",
      "  9.98605264e-20 9.98605264e-20 9.98605264e-20 9.98605264e-20\n",
      "  9.98605264e-20 9.98605264e-20]\n",
      " [1.28677166e-13 1.28676922e-13 1.28677166e-13 1.28686260e-13\n",
      "  1.28679863e-13 1.28692142e-13 1.30070379e-13 1.29298631e-13\n",
      "  9.51072988e-13 4.94981706e-01 4.17856711e-07 5.05017817e-01\n",
      "  1.91139656e-11 1.28692142e-13 1.28677410e-13 1.28676922e-13\n",
      "  1.28676922e-13 1.28676922e-13 1.28676922e-13 1.28676922e-13\n",
      "  1.28676922e-13 1.28676922e-13 1.28676922e-13 1.28805590e-13\n",
      "  1.29946130e-13 1.28894305e-13 1.28712023e-13 1.28707117e-13\n",
      "  1.28677410e-13 1.28678399e-13]\n",
      " [4.57382333e-15 4.57382333e-15 4.57382333e-15 4.58687569e-15\n",
      "  4.57415495e-15 4.57394573e-15 4.67493535e-15 4.57481817e-15\n",
      "  4.57464368e-15 5.01078857e-15 4.57450435e-15 4.57502739e-15\n",
      "  5.42434394e-15 4.57701749e-15 4.58040605e-15 1.11744881e-14\n",
      "  1.24325387e-14 1.24341980e-14 1.93511117e-02 9.80648875e-01\n",
      "  5.48936940e-09 9.64069438e-15 4.57384112e-15 4.57384112e-15\n",
      "  4.57558601e-15 4.57382333e-15 4.57382333e-15 4.57731438e-15\n",
      "  4.57387585e-15 4.57387585e-15]\n",
      " [1.71093498e-15 1.71093498e-15 1.71093498e-15 1.73812357e-15\n",
      "  1.71097415e-15 1.71107199e-15 1.74114970e-15 1.71096759e-15\n",
      "  1.71111116e-15 1.85363504e-15 1.71100031e-15 1.71095446e-15\n",
      "  1.72819984e-15 1.71094154e-15 1.71093498e-15 1.71093498e-15\n",
      "  1.71093498e-15 1.71093498e-15 1.71093498e-15 1.71093498e-15\n",
      "  1.71093498e-15 1.71093498e-15 1.71093498e-15 1.71111116e-15\n",
      "  4.13289228e-14 9.98181343e-01 5.59141222e-09 1.81862246e-03\n",
      "  1.71093498e-15 1.71093498e-15]\n",
      " [9.75910162e-14 9.75910162e-14 9.75910162e-14 9.75910162e-14\n",
      "  9.75910162e-14 9.75910162e-14 9.75917616e-14 9.75910162e-14\n",
      "  9.75910162e-14 9.75913957e-14 9.75910162e-14 9.75910162e-14\n",
      "  9.75910162e-14 9.75910162e-14 9.75910162e-14 9.75990258e-14\n",
      "  9.75910162e-14 7.21960144e-13 6.16790950e-01 3.19026725e-07\n",
      "  3.83208752e-01 9.00855605e-12 9.75910162e-14 9.75910162e-14\n",
      "  9.75910162e-14 9.75910162e-14 9.75910162e-14 9.75910162e-14\n",
      "  9.75910162e-14 9.75910162e-14]\n",
      " [3.35870891e-14 3.35870891e-14 3.35872823e-14 3.35871569e-14\n",
      "  3.35872823e-14 3.36274113e-14 3.36184937e-14 3.36467846e-14\n",
      "  2.40036012e-11 2.70961475e-10 9.72847342e-01 2.71525998e-02\n",
      "  1.35655922e-11 3.35911278e-14 3.35872823e-14 3.35871569e-14\n",
      "  3.35870891e-14 3.35870891e-14 3.35870891e-14 3.35870891e-14\n",
      "  3.35870891e-14 3.35870891e-14 3.35870891e-14 3.35871569e-14\n",
      "  3.35872823e-14 3.36181109e-14 3.36871102e-14 3.36650162e-14\n",
      "  3.35893321e-14 3.37771870e-14]\n",
      " [1.74644408e-15 1.74644408e-15 1.74644408e-15 1.74687702e-15\n",
      "  1.74645065e-15 1.74645732e-15 1.84446464e-15 1.74674372e-15\n",
      "  1.74645065e-15 1.77184321e-15 1.74645732e-15 1.74644408e-15\n",
      "  1.74722356e-15 1.74644408e-15 1.74644408e-15 1.74729694e-15\n",
      "  1.74644408e-15 1.74644408e-15 1.74664388e-15 1.74644408e-15\n",
      "  1.74644408e-15 1.74647743e-15 1.74644408e-15 1.74644408e-15\n",
      "  1.74732373e-15 1.74647055e-15 1.74655716e-15 9.49271619e-01\n",
      "  5.70802339e-09 5.07283211e-02]\n",
      " [1.27176427e-13 1.27176427e-13 1.27176427e-13 1.27176427e-13\n",
      "  1.27176427e-13 1.27176427e-13 1.27176427e-13 1.27176427e-13\n",
      "  1.27176427e-13 1.27176427e-13 1.27176427e-13 1.27178121e-13\n",
      "  1.27193164e-13 1.27183691e-13 9.40054349e-13 5.00285923e-01\n",
      "  4.15535794e-07 4.99713719e-01 1.88585328e-11 1.27177389e-13\n",
      "  1.27176427e-13 1.27176427e-13 1.27176427e-13 1.27176427e-13\n",
      "  1.27176427e-13 1.27176427e-13 1.27176427e-13 1.27189275e-13\n",
      "  1.27231491e-13 1.27195102e-13]\n",
      " [4.60413271e-15 4.60422038e-15 4.60416744e-15 4.60429068e-15\n",
      "  4.60511612e-15 4.60460663e-15 4.60741793e-15 4.61257043e-15\n",
      "  4.60982647e-15 4.60917552e-15 1.25401322e-14 1.25387210e-14\n",
      "  1.79008730e-02 9.82099175e-01 5.51437385e-09 3.40683089e-14\n",
      "  4.61142736e-15 4.60580137e-15 4.60738278e-15 4.60516906e-15\n",
      "  4.60430805e-15 4.60415008e-15 4.60434320e-15 4.60457190e-15\n",
      "  4.61322138e-15 4.60480017e-15 4.60571328e-15 4.62788648e-15\n",
      "  4.60502845e-15 4.60427290e-15]\n",
      " [3.30200548e-14 3.30200548e-14 3.30200548e-14 3.30200548e-14\n",
      "  3.30200548e-14 3.30200548e-14 3.30201192e-14 3.30200548e-14\n",
      "  3.30201192e-14 3.30202445e-14 3.30201192e-14 3.30206206e-14\n",
      "  3.30266684e-14 3.30215659e-14 1.55417051e-11 5.51324320e-10\n",
      "  9.59084868e-01 4.09151167e-02 6.36012900e-12 3.30209357e-14\n",
      "  3.30281185e-14 3.35569382e-14 3.30212508e-14 3.30203699e-14\n",
      "  3.30202445e-14 3.30200548e-14 3.30200548e-14 3.30201192e-14\n",
      "  3.30203055e-14 3.30250320e-14]\n",
      " [1.27292775e-13 1.27292775e-13 1.27292775e-13 1.27292775e-13\n",
      "  1.27292775e-13 1.27292775e-13 1.27293263e-13 1.27293019e-13\n",
      "  1.27295445e-13 1.27330424e-13 1.27304200e-13 9.40808629e-13\n",
      "  4.99709398e-01 4.15938615e-07 5.00290215e-01 1.89109960e-11\n",
      "  1.27336496e-13 1.27293507e-13 1.27293263e-13 1.27292775e-13\n",
      "  1.27292775e-13 1.27292775e-13 1.27292775e-13 1.27292775e-13\n",
      "  1.27292775e-13 1.27292775e-13 1.27292775e-13 1.27308808e-13\n",
      "  1.27293751e-13 1.27292775e-13]\n",
      " [1.70050970e-15 1.70051626e-15 9.26242862e-03 9.90737617e-01\n",
      "  5.55888979e-09 9.06496185e-15 1.70267778e-15 1.70052918e-15\n",
      "  1.70078880e-15 1.70070430e-15 1.70051626e-15 1.70079515e-15\n",
      "  1.70052918e-15 1.70050970e-15 1.70056814e-15 1.70051626e-15\n",
      "  1.70050970e-15 1.70052272e-15 1.70051626e-15 1.70050970e-15\n",
      "  1.70051626e-15 1.70051626e-15 1.70050970e-15 1.70052918e-15\n",
      "  1.70055512e-15 1.70050970e-15 1.70061357e-15 1.70054210e-15\n",
      "  1.70051626e-15 1.70051626e-15]\n",
      " [3.98952391e-15 3.98952391e-15 3.98953916e-15 4.01091785e-15\n",
      "  3.98996564e-15 4.01168272e-15 3.92710708e-09 3.99511306e-15\n",
      "  3.98967595e-15 4.77578224e-15 3.98952391e-15 3.98952391e-15\n",
      "  3.99442738e-15 3.98952391e-15 3.98952391e-15 3.99810139e-15\n",
      "  3.98952391e-15 3.98952391e-15 3.99145726e-15 3.98952391e-15\n",
      "  3.98952391e-15 4.01954106e-15 3.98952391e-15 3.98953916e-15\n",
      "  4.00416064e-15 3.98952391e-15 1.04433708e-14 1.08446916e-14\n",
      "  1.15716137e-01 8.84283900e-01]\n",
      " [4.22810641e-15 8.95769417e-01 1.37010128e-08 1.04230598e-01\n",
      "  1.13114169e-14 4.19646719e-15 4.24059210e-15 4.19569893e-15\n",
      "  4.19560322e-15 4.20075953e-15 4.19569893e-15 4.19560322e-15\n",
      "  4.20451104e-15 4.19613134e-15 4.19585902e-15 4.37081579e-15\n",
      "  4.20353271e-15 4.19619529e-15 4.33665284e-15 4.19906165e-15\n",
      "  4.19661161e-15 4.38563040e-15 4.20109665e-15 4.19697964e-15\n",
      "  4.20561769e-15 4.19958978e-15 4.19616310e-15 4.28814114e-15\n",
      "  4.19721978e-15 4.19584293e-15]\n",
      " [3.38229641e-14 3.38229641e-14 3.40697827e-14 4.21408344e-14\n",
      "  3.38230928e-14 3.38258034e-14 3.38683481e-14 3.38229641e-14\n",
      "  3.38231572e-14 3.38310922e-14 3.38229641e-14 3.38230285e-14\n",
      "  3.38243837e-14 3.38229641e-14 3.38229641e-14 3.38237366e-14\n",
      "  3.38229641e-14 3.38230285e-14 3.38259287e-14 3.38229641e-14\n",
      "  3.68679358e-11 1.01473558e-08 9.82514441e-01 1.74855553e-02\n",
      "  2.95279480e-13 3.38229641e-14 3.38229641e-14 3.38230928e-14\n",
      "  3.38230285e-14 3.47639499e-14]\n",
      " [1.74510069e-15 1.74297178e-15 1.74267256e-15 1.21687921e-14\n",
      "  1.74332425e-15 1.74338407e-15 1.37481660e-13 1.76230022e-15\n",
      "  1.75906286e-15 1.25395402e-10 1.74812565e-15 1.74512070e-15\n",
      "  5.76198919e-13 1.74388954e-15 1.74313801e-15 3.03524008e-15\n",
      "  1.74271248e-15 1.74257949e-15 1.98954931e-15 1.74254635e-15\n",
      "  1.74249310e-15 1.96474332e-15 1.74252634e-15 1.74251311e-15\n",
      "  2.10554751e-15 1.74257949e-15 1.74255292e-15 1.42218366e-12\n",
      "  1.00000000e+00 5.62675329e-09]\n",
      " [2.25647968e-15 1.92178943e-15 1.96094967e-15 2.66444823e-15\n",
      "  9.87495482e-01 6.20074481e-09 1.25045469e-02 2.20262808e-15\n",
      "  1.90969486e-15 1.96865471e-15 1.90985516e-15 1.90866783e-15\n",
      "  1.90944710e-15 1.90868986e-15 1.90866063e-15 1.90879891e-15\n",
      "  1.90866783e-15 1.90866063e-15 1.90934504e-15 1.91000805e-15\n",
      "  1.90916292e-15 3.55913419e-15 2.39386906e-15 1.91896902e-15\n",
      "  1.95640682e-15 1.95395297e-15 1.90866783e-15 1.90933784e-15\n",
      "  1.90879891e-15 1.90872607e-15]]\n",
      "[1.         0.99999994 0.99999994 1.         1.         1.\n",
      " 1.         1.         1.         0.99999994 1.         0.99999994\n",
      " 1.         1.         0.99999994 1.         0.99999994 1.\n",
      " 0.99999994 0.99999994 1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.        ]\n",
      "Test Accuracy: 0.87500\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tflearn\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "\n",
    "from tensorflow.python.platform import gfile\n",
    "from sklearn.utils.extmath import softmax\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "TOKENIZER_RE = re.compile(r\"[A-Z]{2,}(?![a-z])|[A-Z][a-z]+(?=[A-Z])|[\\'\\w\\-]+\", re.UNICODE)\n",
    "MAXLEN = 30\n",
    "\n",
    "SAVE_DIR = \"./save/self-attentive\"\n",
    "SAVE_FILE_PATH = SAVE_DIR + \"/self-attentive.ckpt\"\n",
    "\n",
    "class SelfAttenModel(object):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 batch_size=40, \n",
    "                 vocab_size=200,\n",
    "                 hidden_size=2000,\n",
    "                 label_num=4,\n",
    "                 layer_num=1, \n",
    "                 embedding_size=100, \n",
    "                 keep_prob=0.8, \n",
    "                 max_sequence_length=10,\n",
    "                 num_units=128,\n",
    "                 d_a=350,\n",
    "                 r=30,\n",
    "                 learning_rate=0.01,\n",
    "                 p_coef=0.5,\n",
    "                 use_penalization=True):\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.label_num = label_num\n",
    "        self.layer_num = layer_num\n",
    "        self.embedding_size = embedding_size\n",
    "        self.keep_prob = keep_prob\n",
    "        self.n = self.max_sequence_length = max_sequence_length\n",
    "        self.u = self.num_units = num_units\n",
    "        self.d_a = d_a\n",
    "        self.r = r\n",
    "        self.learning_rate = learning_rate\n",
    "        self.p_coef = p_coef\n",
    "        self.use_penalization = use_penalization\n",
    "        \n",
    "        self._build_placeholder()\n",
    "        self._build_model()\n",
    "        self._build_optimizer()\n",
    "            \n",
    "    def _build_placeholder(self):\n",
    "        self.sources = tf.placeholder(name='sources', shape=[self.batch_size, self.max_sequence_length], dtype=tf.int64)\n",
    "        self.labels = tf.placeholder(name='labels', shape=[self.batch_size], dtype=tf.int64)\n",
    "        self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "    def _build_single_cell(self):\n",
    "        cell = tf.contrib.rnn.BasicLSTMCell(self.num_units)\n",
    "        cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=self.keep_prob)\n",
    "        return cell\n",
    "    \n",
    "    def _build_model(self):\n",
    "        # Word embedding #\n",
    "        with tf.variable_scope(\"embedding\"):\n",
    "            initializer = tf.contrib.layers.xavier_initializer()\n",
    "            embeddings = tf.get_variable(name=\"embedding_encoder\",\n",
    "                                                shape=[self.vocab_size, self.embedding_size], \n",
    "                                                dtype=tf.float32,\n",
    "                                                initializer=initializer,\n",
    "                                                trainable=True)\n",
    "\n",
    "            input_embeddings = tf.nn.embedding_lookup(params=embeddings,\n",
    "                                                      ids=self.sources)\n",
    "\n",
    "        # Bidirectional rnn #\n",
    "        with tf.variable_scope(\"bidirectional_rnn\"):\n",
    "            cell_forward = self._build_single_cell()\n",
    "            cell_backward = self._build_single_cell()\n",
    "            \n",
    "            # outputs is state 'H'\n",
    "            outputs, _ = tf.nn.bidirectional_dynamic_rnn(cell_fw=cell_forward, \n",
    "                                                              cell_bw=cell_backward, \n",
    "                                                              inputs=input_embeddings,\n",
    "                                                              dtype=tf.float32)\n",
    "            \n",
    "            H = tf.concat(outputs, -1)\n",
    "            \n",
    "        # Self Attention #\n",
    "        with tf.variable_scope(\"self_attention\"):\n",
    "            initializer = tf.contrib.layers.xavier_initializer()\n",
    "            W_s1 = tf.get_variable(name=\"W_s1\", shape=[self.d_a, 2*self.u], initializer=initializer)\n",
    "            W_s2 = tf.get_variable(name='W_s2', shape=[self.r, self.d_a],initializer=initializer)\n",
    "            \n",
    "            a_prev = tf.map_fn(lambda x: tf.matmul(W_s1, tf.transpose(x)), H)\n",
    "            a_prev = tf.tanh(a_prev)\n",
    "            a_prev = tf.map_fn(lambda x: tf.matmul(W_s2, x), a_prev)\n",
    "            \n",
    "            self.A = tf.nn.softmax(a_prev)\n",
    "            self.M = tf.matmul(self.A, H)\n",
    "        \n",
    "        # Fully connected layer #\n",
    "        with tf.variable_scope(\"fully_connected_layer\"):\n",
    "            input_fc = tf.layers.flatten(self.M)\n",
    "            layer_fc = tf.contrib.layers.fully_connected(inputs=input_fc, \n",
    "                                                         num_outputs=self.hidden_size,\n",
    "                                                         activation_fn=tf.nn.relu)\n",
    "            \n",
    "            self.logits = tf.contrib.layers.fully_connected(inputs=layer_fc, \n",
    "                                                            num_outputs=self.label_num,\n",
    "                                                            activation_fn=None)\n",
    "            \n",
    "            \n",
    "            \n",
    "    def _build_optimizer(self):\n",
    "        with tf.variable_scope(\"optimizer\"):\n",
    "            cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits,\n",
    "                                                                           labels=self.labels)\n",
    "            self.loss = tf.reduce_mean(cross_entropy)\n",
    "            \n",
    "            if self.use_penalization:\n",
    "                A_T = tf.transpose(self.A, perm=[0, 2, 1])\n",
    "                tile_eye = tf.tile(tf.eye(self.r), [self.batch_size, 1])\n",
    "                tile_eye = tf.reshape(tile_eye, [-1, self.r, self.r])\n",
    "                AA_T = tf.matmul(self.A, A_T) - tile_eye\n",
    "                P = tf.square(tf.norm(AA_T, axis=[-2, -1], ord='fro'))\n",
    "                p_loss = self.p_coef * P\n",
    "                self.loss = self.loss + p_loss\n",
    "            \n",
    "            self.loss = tf.reduce_mean(self.loss)\n",
    "            \n",
    "            params = tf.trainable_variables()\n",
    "            optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "            grad_and_vars = tf.gradients(self.loss, params)\n",
    "            clipped_gradients, _ = tf.clip_by_global_norm(grad_and_vars, 0.5)\n",
    "            self.optimizer = optimizer.apply_gradients(zip(clipped_gradients, params), global_step=self.global_step)\n",
    "            \n",
    "            self.predict = tf.argmax(self.logits, -1)\n",
    "            self.correct_pred = tf.equal(self.predict, self.labels)\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(self.correct_pred, tf.float32))\n",
    "            \n",
    "\n",
    "def load_csv(filepath, target_column=-1, data_column=None, has_header=False):\n",
    "    with gfile.Open(filepath) as csv_file:\n",
    "        data_file = csv.reader(csv_file)\n",
    "        if has_header:\n",
    "            header = next(data_file)\n",
    "\n",
    "        data, target = [], []\n",
    "        for i, row in enumerate(data_file):\n",
    "            data.append([_d for _i, _d in enumerate(row) if _i == data_column])\n",
    "            target.append([int(_d) for _i, _d in enumerate(row) if _i == target_column])\n",
    "\n",
    "        return data, target\n",
    "\n",
    "def token_parse(iterator):\n",
    "    for value in iterator:\n",
    "        return TOKENIZER_RE.findall(value)\n",
    "\n",
    "\n",
    "def string_parser(arr, tokenizer, fit):\n",
    "    if fit == False:\n",
    "        return list(tokenizer.transform(arr)), tokenizer\n",
    "    else:\n",
    "        return list(tokenizer.fit_transform(arr)), tokenizer\n",
    "\n",
    "def main():\n",
    "    # Set mode\n",
    "    is_training = False\n",
    "    \n",
    "    # Preparing data\n",
    "    tokenizer = tflearn.data_utils.VocabularyProcessor(MAXLEN, tokenizer_fn=lambda tokens: [token_parse(x) for x in tokens])\n",
    "    sources_raw, labels = load_csv('./data/ag_news_csv/train.csv', target_column=0, data_column=2)\n",
    "    sources, vocab_processor = string_parser(sources_raw, tokenizer, fit=True)\n",
    "    sources = tflearn.data_utils.pad_sequences(sources, maxlen=MAXLEN)\n",
    "    labels = np.squeeze(labels)\n",
    "        \n",
    "    sources, labels = shuffle(sources, labels)\n",
    "    vocab_size = len(vocab_processor.vocabulary_._mapping)\n",
    "    label_num = int(np.max(labels) + 1)\n",
    "    \n",
    "    # Training options \n",
    "    epoch_nums = 1\n",
    "    batch_size = 80\n",
    "    total = len(sources)\n",
    "    step_nums = int(total/batch_size)\n",
    "    display_step = int(step_nums / 100)\n",
    "     \n",
    "    if is_training == True:\n",
    "        keep_prob = 0.8\n",
    "    else:\n",
    "        keep_prob = 1.0\n",
    "        \n",
    "    model = SelfAttenModel(batch_size=batch_size,\n",
    "                           vocab_size=vocab_size,\n",
    "                           label_num=label_num,\n",
    "                           keep_prob=keep_prob,\n",
    "                           learning_rate=0.01,\n",
    "                           p_coef=0.25,\n",
    "                           max_sequence_length=MAXLEN)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Saver\n",
    "        saver = tf.train.Saver()\n",
    "        ckpt_path = tf.train.latest_checkpoint(checkpoint_dir=SAVE_DIR)\n",
    "        if ckpt_path: \n",
    "            saver.restore(sess, ckpt_path)\n",
    "        else:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "        # train mode\n",
    "        if is_training == True:\n",
    "            for epoch in range(epoch_nums):\n",
    "                print(\"%d Epoch Start\" % epoch)\n",
    "                display_loss = []\n",
    "                display_accuracy = []\n",
    "                for step in range(step_nums):\n",
    "\n",
    "                    batch_start = step * batch_size\n",
    "                    batch_end = batch_start + batch_size\n",
    "                    batch_sources, batch_labels = (sources[batch_start:batch_end], labels[batch_start:batch_end])\n",
    "\n",
    "                    loss, accuracy, predict, _= sess.run([model.loss, model.accuracy, model.predict, model.optimizer], \n",
    "                                                feed_dict={model.sources: batch_sources, \n",
    "                                                           model.labels: batch_labels})\n",
    "                    display_loss.append(loss)\n",
    "                    display_accuracy.append(accuracy)\n",
    "\n",
    "                    if (step % display_step) == 0:\n",
    "                        \n",
    "                        print(\"Step \" + str(step * batch_size) + \", Minibatch Loss= \" + \\\n",
    "                              \"{:.6f}\".format(np.mean(display_loss)) + \", Training Accuracy= \" + \\\n",
    "                              \"{:.5f}\".format(np.mean(display_accuracy)))\n",
    "                        \n",
    "                        display_loss.clear()\n",
    "                        display_accuracy.clear()\n",
    "\n",
    "                        saver.save(sess, SAVE_FILE_PATH)\n",
    "                        \n",
    "                        test_sources_raw, test_labels = load_csv('./data/ag_news_csv/test.csv', target_column=0, data_column=2)\n",
    "                        test_sources, vocab_processor = string_parser(test_sources_raw, tokenizer, fit=True)\n",
    "                        test_sources = tflearn.data_utils.pad_sequences(test_sources, maxlen=MAXLEN)\n",
    "                        test_labels = np.squeeze(test_labels)\n",
    "\n",
    "                        test_total = len(test_sources)\n",
    "                        test_step_nums = int(test_total/batch_size)\n",
    "\n",
    "                        accuracy_list = []\n",
    "                        for step in range(test_step_nums):\n",
    "\n",
    "                            batch_start = step * batch_size\n",
    "                            batch_end = batch_start + batch_size\n",
    "                            batch_sources, batch_labels = (test_sources[batch_start:batch_end], test_labels[batch_start:batch_end])\n",
    "\n",
    "                            A, accuracy = sess.run([model.A, model.accuracy], \n",
    "                                                                feed_dict={model.sources: batch_sources,\n",
    "                                                                           model.labels: batch_labels})\n",
    "                            accuracy_list.append(accuracy)\n",
    "\n",
    "                        print(\"Test Accuracy: {:.5f}\".format(np.mean(accuracy_list)))\n",
    "\n",
    "            print(\"Optimization Finished!\")\n",
    "        \n",
    "        # test mode\n",
    "        else:\n",
    "            \n",
    "            test_sources_raw, test_labels = load_csv('./data/ag_news_csv/test.csv', target_column=0, data_column=2)\n",
    "            test_sources, vocab_processor = string_parser(test_sources_raw, tokenizer, fit=True)\n",
    "            test_sources = tflearn.data_utils.pad_sequences(test_sources, maxlen=MAXLEN)\n",
    "            test_labels = np.squeeze(test_labels)\n",
    "            \n",
    "            test_total = len(test_sources)\n",
    "            test_step_nums = int(test_total/batch_size)\n",
    "    \n",
    "            accuracy_list = []\n",
    "            step = 0 #for step in range(test_step_nums):\n",
    "    \n",
    "            batch_start = step * batch_size\n",
    "            batch_end = batch_start + batch_size\n",
    "            batch_sources, batch_labels = (test_sources[batch_start:batch_end], test_labels[batch_start:batch_end])\n",
    "\n",
    "            A, accuracy = sess.run([model.A, model.accuracy], \n",
    "                                                feed_dict={model.sources: batch_sources,\n",
    "                                                           model.labels: batch_labels})\n",
    "            accuracy_list.append(accuracy)\n",
    "            a_sum = np.sum(A, axis=0)\n",
    "            \n",
    "            print(softmax(a_sum))\n",
    "            print(\"Test Accuracy: {:.5f}\".format(np.mean(accuracy_list)))\n",
    "            \n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
