{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3, Minibatch Loss= 2.471797, Training Accuracy= 0.20000\n",
      "Step 300, Minibatch Loss= 2.420745, Training Accuracy= 0.20000\n",
      "Step 600, Minibatch Loss= 2.277771, Training Accuracy= 0.26667\n",
      "Step 900, Minibatch Loss= 2.045573, Training Accuracy= 0.26667\n",
      "Step 1200, Minibatch Loss= 1.836610, Training Accuracy= 0.40000\n",
      "Step 1500, Minibatch Loss= 1.713699, Training Accuracy= 0.46667\n",
      "Step 1800, Minibatch Loss= 1.594489, Training Accuracy= 0.53333\n",
      "Step 2100, Minibatch Loss= 1.497533, Training Accuracy= 0.66667\n",
      "Step 2400, Minibatch Loss= 1.442820, Training Accuracy= 0.60000\n",
      "Step 2700, Minibatch Loss= 1.409559, Training Accuracy= 0.60000\n",
      "Step 3000, Minibatch Loss= 1.385251, Training Accuracy= 0.60000\n",
      "Step 3300, Minibatch Loss= 1.366181, Training Accuracy= 0.60000\n",
      "Step 3600, Minibatch Loss= 1.350595, Training Accuracy= 0.60000\n",
      "Step 3900, Minibatch Loss= 1.337040, Training Accuracy= 0.60000\n",
      "Step 4200, Minibatch Loss= 1.324590, Training Accuracy= 0.60000\n",
      "Step 4500, Minibatch Loss= 1.313203, Training Accuracy= 0.60000\n",
      "Step 4800, Minibatch Loss= 1.302736, Training Accuracy= 0.60000\n",
      "Step 5100, Minibatch Loss= 1.293163, Training Accuracy= 0.60000\n",
      "Step 5400, Minibatch Loss= 1.284860, Training Accuracy= 0.60000\n",
      "Step 5700, Minibatch Loss= 1.277721, Training Accuracy= 0.60000\n",
      "Step 6000, Minibatch Loss= 1.271393, Training Accuracy= 0.60000\n",
      "Step 6300, Minibatch Loss= 1.265585, Training Accuracy= 0.60000\n",
      "Step 6600, Minibatch Loss= 1.260003, Training Accuracy= 0.60000\n",
      "Step 6900, Minibatch Loss= 1.254393, Training Accuracy= 0.60000\n",
      "Step 7200, Minibatch Loss= 1.248775, Training Accuracy= 0.60000\n",
      "Step 7500, Minibatch Loss= 1.243416, Training Accuracy= 0.60000\n",
      "Step 7800, Minibatch Loss= 1.238516, Training Accuracy= 0.60000\n",
      "Step 8100, Minibatch Loss= 1.234094, Training Accuracy= 0.60000\n",
      "Step 8400, Minibatch Loss= 1.230131, Training Accuracy= 0.66667\n",
      "Step 8700, Minibatch Loss= 1.226591, Training Accuracy= 0.66667\n",
      "Step 9000, Minibatch Loss= 1.223326, Training Accuracy= 0.66667\n",
      "Step 9300, Minibatch Loss= 1.218576, Training Accuracy= 0.66667\n",
      "Step 9600, Minibatch Loss= 1.212507, Training Accuracy= 0.66667\n",
      "Step 9900, Minibatch Loss= 1.208235, Training Accuracy= 0.66667\n",
      "Step 10200, Minibatch Loss= 1.204862, Training Accuracy= 0.66667\n",
      "Step 10500, Minibatch Loss= 1.202034, Training Accuracy= 0.66667\n",
      "Step 10800, Minibatch Loss= 1.199571, Training Accuracy= 0.66667\n",
      "Step 11100, Minibatch Loss= 1.197376, Training Accuracy= 0.66667\n",
      "Step 11400, Minibatch Loss= 1.195386, Training Accuracy= 0.66667\n",
      "Step 11700, Minibatch Loss= 1.193550, Training Accuracy= 0.66667\n",
      "Step 12000, Minibatch Loss= 1.191827, Training Accuracy= 0.66667\n",
      "Step 12300, Minibatch Loss= 1.190177, Training Accuracy= 0.66667\n",
      "Step 12600, Minibatch Loss= 1.188580, Training Accuracy= 0.66667\n",
      "Step 12900, Minibatch Loss= 1.187026, Training Accuracy= 0.66667\n",
      "Step 13200, Minibatch Loss= 1.185489, Training Accuracy= 0.66667\n",
      "Step 13500, Minibatch Loss= 1.183940, Training Accuracy= 0.66667\n",
      "Step 13800, Minibatch Loss= 1.182383, Training Accuracy= 0.66667\n",
      "Step 14100, Minibatch Loss= 1.180460, Training Accuracy= 0.73333\n",
      "Step 14400, Minibatch Loss= 1.173490, Training Accuracy= 0.73333\n",
      "Step 14700, Minibatch Loss= 1.170567, Training Accuracy= 0.73333\n",
      "Step 15000, Minibatch Loss= 1.168318, Training Accuracy= 0.73333\n",
      "Step 15300, Minibatch Loss= 1.166293, Training Accuracy= 0.73333\n",
      "Step 15600, Minibatch Loss= 1.164364, Training Accuracy= 0.73333\n",
      "Step 15900, Minibatch Loss= 1.162526, Training Accuracy= 0.73333\n",
      "Step 16200, Minibatch Loss= 1.160822, Training Accuracy= 0.73333\n",
      "Step 16500, Minibatch Loss= 1.159294, Training Accuracy= 0.73333\n",
      "Step 16800, Minibatch Loss= 1.157928, Training Accuracy= 0.73333\n",
      "Step 17100, Minibatch Loss= 1.156678, Training Accuracy= 0.73333\n",
      "Step 17400, Minibatch Loss= 1.155497, Training Accuracy= 0.73333\n",
      "Step 17700, Minibatch Loss= 1.154350, Training Accuracy= 0.73333\n",
      "Step 18000, Minibatch Loss= 1.153202, Training Accuracy= 0.73333\n",
      "Step 18300, Minibatch Loss= 1.151996, Training Accuracy= 0.73333\n",
      "Step 18600, Minibatch Loss= 1.150840, Training Accuracy= 0.73333\n",
      "Step 18900, Minibatch Loss= 1.149716, Training Accuracy= 0.73333\n",
      "Step 19200, Minibatch Loss= 1.147919, Training Accuracy= 0.73333\n",
      "Step 19500, Minibatch Loss= 1.143177, Training Accuracy= 0.73333\n",
      "Step 19800, Minibatch Loss= 1.140476, Training Accuracy= 0.80000\n",
      "Step 20100, Minibatch Loss= 1.138894, Training Accuracy= 0.80000\n",
      "Step 20400, Minibatch Loss= 1.137463, Training Accuracy= 0.80000\n",
      "Step 20700, Minibatch Loss= 1.135379, Training Accuracy= 0.80000\n",
      "Step 21000, Minibatch Loss= 1.129458, Training Accuracy= 0.86667\n",
      "Step 21300, Minibatch Loss= 1.123702, Training Accuracy= 0.86667\n",
      "Step 21600, Minibatch Loss= 1.119011, Training Accuracy= 0.86667\n",
      "Step 21900, Minibatch Loss= 1.115481, Training Accuracy= 0.86667\n",
      "Step 22200, Minibatch Loss= 1.113059, Training Accuracy= 0.86667\n",
      "Step 22500, Minibatch Loss= 1.111218, Training Accuracy= 0.86667\n",
      "Step 22800, Minibatch Loss= 1.109722, Training Accuracy= 0.86667\n",
      "Step 23100, Minibatch Loss= 1.108462, Training Accuracy= 0.86667\n",
      "Step 23400, Minibatch Loss= 1.107377, Training Accuracy= 0.86667\n",
      "Step 23700, Minibatch Loss= 1.106426, Training Accuracy= 0.86667\n",
      "Step 24000, Minibatch Loss= 1.105581, Training Accuracy= 0.86667\n",
      "Step 24300, Minibatch Loss= 1.104822, Training Accuracy= 0.86667\n",
      "Step 24600, Minibatch Loss= 1.104135, Training Accuracy= 0.86667\n",
      "Step 24900, Minibatch Loss= 1.103509, Training Accuracy= 0.86667\n",
      "Step 25200, Minibatch Loss= 1.102935, Training Accuracy= 0.86667\n",
      "Step 25500, Minibatch Loss= 1.102407, Training Accuracy= 0.86667\n",
      "Step 25800, Minibatch Loss= 1.101918, Training Accuracy= 0.86667\n",
      "Step 26100, Minibatch Loss= 1.101466, Training Accuracy= 0.86667\n",
      "Step 26400, Minibatch Loss= 1.101046, Training Accuracy= 0.86667\n",
      "Step 26700, Minibatch Loss= 1.100656, Training Accuracy= 0.86667\n",
      "Step 27000, Minibatch Loss= 1.100292, Training Accuracy= 0.86667\n",
      "Step 27300, Minibatch Loss= 1.099953, Training Accuracy= 0.86667\n",
      "Step 27600, Minibatch Loss= 1.099636, Training Accuracy= 0.86667\n",
      "Step 27900, Minibatch Loss= 1.099340, Training Accuracy= 0.86667\n",
      "Step 28200, Minibatch Loss= 1.099063, Training Accuracy= 0.86667\n",
      "Step 28500, Minibatch Loss= 1.098804, Training Accuracy= 0.86667\n",
      "Step 28800, Minibatch Loss= 1.098561, Training Accuracy= 0.86667\n",
      "Step 29100, Minibatch Loss= 1.098332, Training Accuracy= 0.86667\n",
      "Step 29400, Minibatch Loss= 1.098116, Training Accuracy= 0.86667\n",
      "Step 29700, Minibatch Loss= 1.097911, Training Accuracy= 0.86667\n",
      "Step 30000, Minibatch Loss= 1.097716, Training Accuracy= 0.86667\n",
      "Step 30300, Minibatch Loss= 1.097527, Training Accuracy= 0.86667\n",
      "Step 30600, Minibatch Loss= 1.097338, Training Accuracy= 0.86667\n",
      "Step 30900, Minibatch Loss= 1.097133, Training Accuracy= 0.86667\n",
      "Step 31200, Minibatch Loss= 1.096819, Training Accuracy= 0.93333\n",
      "Step 31500, Minibatch Loss= 1.095867, Training Accuracy= 0.93333\n",
      "Step 31800, Minibatch Loss= 1.093257, Training Accuracy= 1.00000\n",
      "           ->  ['what', 'is', 'your', 'hobby', '</s>']\n",
      "           ->  ['nice', 'to', 'meet', 'you', '</s>']\n",
      "           ->  ['meet', 'tomorrow', '</s>', '<p>', '<p>']\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "TOKEN_PAD=\"<p>\"\n",
    "TOKEN_START=\"<s>\"\n",
    "TOKEN_END=\"</s>\"\n",
    "\n",
    "source_sentences = [\"취미가 뭐예요\", \"만나서 반가워요\", \"내일 만나서 놀아요\"]\n",
    "target_sentences = [\"what is your hobby\", \"nice to meet you\", \"meet tomorrow\"]\n",
    "\n",
    "source_vocab = [TOKEN_PAD, TOKEN_START, TOKEN_END, \"취미가\", \"뭐예요\", \"만나서\", \"반가워요\", \"내일\", \"놀아요\"]\n",
    "target_vocab = [TOKEN_PAD, TOKEN_START, TOKEN_END, \"what\", \"is\", \"your\", \"hobby\", \"nice\", \"to\", \"meet\", \"you\", \"tomorrow\"]\n",
    "\n",
    "source_input_idx = [\n",
    "    [3,  4,  0,  0,  0], \n",
    "    [5,  6,  0,  0,  0],  \n",
    "    [7,  5,  8,  0,  0]]\n",
    "target_input_idx = [\n",
    "    [1,  3,  4,  5,  6], \n",
    "    [1,  7,  8,  9, 10], \n",
    "    [1,  9, 11,  0,  0]]\n",
    "target_output_idx = [\n",
    "    [3,  4,  5,  6,  2], \n",
    "    [7,  8,  9, 10,  2], \n",
    "    [9, 11,  2,  0,  0]]\n",
    "\n",
    "sen_num = len(source_sentences)\n",
    "source_vocab_size = len(source_vocab)\n",
    "target_vocab_size = len(target_vocab)\n",
    "max_vocab_size = max(source_vocab_size, target_vocab_size)\n",
    "\n",
    "source_input_one_hot = []\n",
    "target_input_one_hot = []\n",
    "\n",
    "for sen_idx in range(sen_num):\n",
    "    source_input_one_hot.append(np.eye(max_vocab_size)[source_input_idx[sen_idx]])\n",
    "    target_input_one_hot.append(np.eye(max_vocab_size)[target_input_idx[sen_idx]])\n",
    "\n",
    "num_units = 12\n",
    "num_layer = 2\n",
    "\n",
    "batch_size = 3\n",
    "learning_rate = 0.0002\n",
    "\n",
    "training_steps = 20000\n",
    "display_step = 100\n",
    "\n",
    "max_sentence_length = 5\n",
    "                   \n",
    "source_inputs = tf.placeholder(dtype=tf.float32, shape=(batch_size, max_sentence_length, max_vocab_size), name='source_inputs')\n",
    "target_inputs = tf.placeholder(dtype=tf.float32, shape=(batch_size, max_sentence_length, max_vocab_size), name='target_inputs')\n",
    "target_outputs = tf.placeholder(dtype=tf.int64, shape=(batch_size, max_sentence_length), name='target_outputs')\n",
    "\n",
    "def build_single_cell(num_units):\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(num_units)\n",
    "    return cell\n",
    "                   \n",
    "with tf.variable_scope('encoder'):\n",
    "    \n",
    "    encoder_cell_list = [build_single_cell(num_units) for i in range(num_layer)]\n",
    "    encoder_multi_cell = tf.contrib.rnn.MultiRNNCell(encoder_cell_list)\n",
    "    \n",
    "    encoder_outputs, encoder_final_state = tf.nn.dynamic_rnn(cell=encoder_multi_cell,\n",
    "                                                             inputs=source_inputs,\n",
    "                                                             dtype=tf.float32)\n",
    "    \n",
    "with tf.variable_scope('decoder'):\n",
    "    \n",
    "    decoder_cell_list = [build_single_cell(num_units) for i in range(num_layer)]\n",
    "    decoder_multi_cell = tf.contrib.rnn.MultiRNNCell(decoder_cell_list)\n",
    "    \n",
    "    decoder_initial_state = encoder_final_state\n",
    "    decoder_outputs, decoder_final_state = tf.nn.dynamic_rnn(cell=decoder_multi_cell,\n",
    "                                                             inputs=target_inputs,\n",
    "                                                             initial_state=decoder_initial_state,\n",
    "                                                             dtype=tf.float32)\n",
    "\n",
    "    decoder_predict = tf.argmax(decoder_outputs, 2)\n",
    "    \n",
    "with tf.variable_scope(\"optimizer\"):\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=decoder_outputs,\n",
    "                                                                   labels=target_outputs)\n",
    "    cost = tf.reduce_mean(cross_entropy)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    optimizer = optimizer.minimize(cost)\n",
    "    \n",
    "    correct_pred = tf.equal(tf.argmax(decoder_outputs, 2), target_outputs)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    \n",
    "    \n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(1, training_steps + 1):\n",
    "        batch_source_input = source_input_idx\n",
    "        batch_target_input = target_input_idx\n",
    "        batch_target_output = target_output_idx\n",
    "        \n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(optimizer, feed_dict={source_inputs: source_input_one_hot, \n",
    "                                       target_inputs: target_input_one_hot,\n",
    "                                       target_outputs: target_output_idx})\n",
    "        \n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch accuracy & loss\n",
    "            outputs, acc, loss = sess.run([decoder_predict, accuracy, cost], \n",
    "                                              feed_dict={source_inputs: source_input_one_hot, \n",
    "                                                         target_inputs: target_input_one_hot,\n",
    "                                                         target_outputs: target_output_idx})\n",
    "            \n",
    "            print(\"Step \" + str(step * batch_size) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc))\n",
    "            \n",
    "        if acc >= 1:\n",
    "            for sentence in outputs:\n",
    "                sentence = [target_vocab[word_idx] for word_idx in sentence]\n",
    "                print(\"           -> \", sentence)\n",
    "                \n",
    "            break;\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={source_inputs: source_input_one_hot, \n",
    "                                                             target_inputs: target_input_one_hot,\n",
    "                                                             target_outputs: target_output_idx}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
